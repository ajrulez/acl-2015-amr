%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,url,bbm,rotating,datetime}
\usepackage{enumitem,multirow,hhline,stmaryrd,bussproofs,mathtools,siunitx,arydshln}

\input std-macros.tex

\newcommand\w[1]{\textit{#1}} % A textual phrase
\newcommand\e[1]{\textit{#1}} % An AMR edge
\newcommand\n[1]{\textit{#1}} % An AMR node

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Robust Subgraph Generation Improves Abstract Meaning Representation Parsing}
\author{
%  Keenon Werling \\
%  Stanford University\\
%  {\tt keenon@stanford.edu} \\\And
%  Gabor Angeli \\
%  Stanford University\\
%  {\tt angeli@cs.stanford.edu} \\\And
%  Chris Manning \\
%  Stanford University\\
%  {\tt manning@cs.stanford.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

% The existing AMR corpus is heavily biased to the international newswire domain, making domain transfer of AMR parsing an important challenge. 
% The Abstract Meaning Representation (AMR) is a semantic formalism for which a large and growing set of annotated examples is available.

Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like semantic parsing
  and machine translation.
Node generation, typically done using a simple
  dictionary lookup, is currently the crucial limiting factor in AMR parsing.
% The state-of-the-art AMR parser cannot generate nodes for tokens unseen at training time, limiting its generalization.
We propose a small set of actions that derive AMR sub-graphs by transformations on spans of text, which allows for more robust learning of this stage.
%the first AMR parser that can handle words at test time that were unseen during training.
Our set of construction actions generalize better than the previous approach, 
  and can be learned with a simple classifier.
% These actions also provide an insight for an alignment system that yields a ``maximally informative'' set of action labels, which we show yields good results.
We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end F$_1$ from 0.59 to 0.62 on the LDC2013E117 and LDC2014T12 datasets.

\end{abstract}

\section{Introduction}

\Fig{glee.png}{0.20}{glee}{The AMR graph for 
\w{He gleefully ran to his dog Rover}.
We show that improving the generation of low level subgraphs
  (e.g., \w{Rover} generating \n{name} $\xrightarrow{:op1}$ \n{``Rover''}) significantly improves
  end-to-end performance.
}

Abstract Meaning Representation (AMR) \cite{2013banarescu-amr} is a rich, graph-based language for expressing semantics over a broad domain.
AMR parsing is the task of mapping a natural language sentence into an AMR graph.
% AMR is written as a directed, rooted graph merging propositional logic and neo-Davidsonian semantics (\todo{cite}).
The project is backed by a large data-labeling effort, and the formalism  hold
%parsing 
%is exciting because a practical broad-domain AMR parser
  promise for enabling a new breed of natural language applications ranging from semantically aware MT to rich broad-domain QA over text-based knowledge bases.
\reffig{glee} shows an example AMR for ``he gleefully ran to his dog Rover,'' and we give a brief tutorial on AMR in \refsec{crash}.


%It's an exciting time for AMR parsing research, because at the time of this writing 
%AMR is the target 
  %of a multi-institution multi-year data-labeling program led by Kevin Knight, which 
  %promises to produce a corpus that is large enough to make it possible to parse to AMR 
  %well despite its many challenges.

\FigStar{method.png}{0.22}{method}{
A graphical explanation of our method. We represent the derivation process for \w{He gleefully ran to his dog Rover}. First the tokens in the sentence are labeled with derivation actions, then those actions are used to generate AMR sub-graphs, and then those sub-graphs are stitched together to form a coherent whole.}

% AMR parsing remains a hard unsolved problem, with only modest performance numbers reported so far.
% A successful AMR parser needs to be able to handle a broad range of semantic interpretation tasks.

%While part of the appeal of AMR is the ability to get away to a certain extent with lexical memorization
%as a substitute for deep parsing, this approach suffers on generalization in the absence of a huge dataset.
% This paper proposes a method for de-lexicalizing ... \todo{words}
% AMR also possesses distinctive computational challenges in its structural properties.
% For example, it provides no guarantees about projectivity or even acyclicity of its graphs.

We follow previous work \cite{2014flanigan-amr} in dividing AMR parsing into two steps. 
The first step is \textit{concept identification}, which generates concept (or entity) nodes from text, and which we'll refer to as \textit{NER++} (\refsec{ner}). 
The second step is \textit{relation identification}, which adds arcs to link these nodes into a fully connected AMR graph, which we'll call \textit{SRL++} (\refsec{srl}).

We make the observation that SRL++ is not the hard part of AMR parsing;
  rather, much of the difficulty in AMR is generating high accuracy concept
  sub-graphs from the NER++ component.
%The hard part of AMR parsing is NER++. 
%The evidence for this claim comes from a close reading of the only published AMR parser to date, \newcite{2014flanigan-amr}, dubbed ``JAMR''.
%When JAMR parses using its own NER++ and SRL++ components, it gets an end-to-end score of 0.58 F$_1$.
For example, when JAMR is given a gold NER++ output, and must only perform SRL++ 
  over given sub-graphs it scores 80 F$_1$ -- nearly the inter-annotator agreement of 83 F$_1$, and far higher than the end to end accuracy of 59 F$_1$.
%, which is inter-annotator agreement on AMR \cite{2013banarescu-amr}  \footnote{Keeping in mind that annotators can sometimes disagree about NER++, this means that the IAA is not a perfect upper bound for parsing performance when given perfect NER++}.
%This means the SRL++ component is quite good, if given perfect NER++. A bad NER++ system is costing \cite{2014flanigan-amr} more than 20 F$_1$.

The primary contribution of this paper is a method to largely delexicalize NER++ to gracefully handle unseen tokens.
Choosing from among a small set of \textit{generative actions} our system can derive an AMR sub-graph from a span of tokens (see \reffig{method}).
For example, we have an action \textit{VERB} that will perform a verb-sense-disambiguation to the appropriate PropBank frame \cite{palmer2005proposition-srl} on the source token, like from \w{ran} to \n{run-01} in \reffig{glee}.

In retrospect, it makes sense that SRL++ within AMR is \textit{relatively} easy given a perfect NER++ output, because NER++ carries so much information.
There's a strong type-check feature for the existence and type of any arc just by looking at its end-points, and syntactic dependency features are very informative for removing any remaining ambiguity.
If a system is considering how to link the node ``run-01'' in \reffig{glee}, the verb-sense frame for ``run-01'' leaves very little entropy in terms of what we could assign as an \w{ARG0} arc.
It must be a noun, which leaves either ``he'' or ``dog'', and this is easily decided in favor of ``he'' by looking for an nsubj arc in the dependency parse. 
%%Given a modest amount of data, a simple parser can learn these lexical and syntactic type-check rules.
%
%While SRL++ is hard, NER++ is extremely challenging, encompassing many SemEval tasks and a rich set of lemmatizations, nominalizations, and functional shifts.
%This task was handled by a simple baseline system in \cite{2014flanigan-amr}, which memorized a purely lexical mapping from spans of text to AMR nodes, augmented by an NER system and time expression regex at test time.
%This is problematic, because 38\% of the tokens in the dev set are unobserved at training time, and the existing method has no way to handle generation for them.

Our approach to the end-to-end AMR parsing task is therefore as follows:
  we define an action space for generating AMR concepts, and create a classifier
  for classifying lexical items into one of these actions (\refsec{nerplusplus}).
This classifier is trained from automatically generated alignments between the
  gold AMR trees and their associated sentences (\refsec{alignment}), using an
  objective which favors alignment mistakes which are least harmful to the NER++
  component.
Finally, the concept subgraphs are combined into a coherent AMR parse using the
  MST-based algorithm of \newcite{2014flanigan-amr}.


We show that our approach provides a large boost to recall over previous approaches, and that end to end performance is improved from 59 to 62 \n{smatch} (an F$_1$ measure of correct AMR arcs; see \cite{cai2013smatch-amr}) when incorporated into the SRL++ parser of \newcite{2014flanigan-amr}.
%our generative actions are stitched together by the previous state of the art parser \cite{2014flanigan-amr}.
Our system is able to correctly generates parses for 77\% of tokens in the dev set, where the baseline approach previously in place can only handle 63\%: a gain of 14\%.
% ?????? keep or throw out this defensive sentence ??????
%We suspect that generating so many more correct nodes would result in a higher end-to-end boost if the SRL++ component were less reliant on lexicalized features, which we leave to future work.

% Previous work has observed that AMR parsing can be partitioned into two tasks: a rich lexically grounded entity detection system, which we call NER++ (see \refsec{ner}), and a relationship detection system, which we call SRL++ (see \refsec{srl}).

% As an example of the distinction, let's briefly go over \reffig{glee}, the AMR for ``he gleefully ran to his dog Rover''. To produce this parse, the NER++ task will have to do a verb sense disambiguation on ``ran'' to get ``run-01'', and a lemmatization on ``gleefully'' to ``glee''. Then NER++ will have to recognize ``Rover'' is a name, and generate the sub-graph (name :op1 ``Rover''). It will also have to recognize that ``to'' is a dropped preposition, and so doesn't get a node, and ``his'' is a coreferant that also doesn't get a node. Then ``dog'' and ``he'' will have to be generated directly as nodes. The SRL++ task is then responsible for linking together the output of the NER++ task to produce a fully-connected semantic graph.

% At test time, AMR parsing is commonly divided into two tasks, which we refer to as NER++ and SRL++.

% Valid examples to use:
% excessively -> excessive
% mother -> mother
% exhaustive -> exhaust-01

% For instance, the adjective \textit{exhaustive} in ``the \textit{exhaustive} search'' should be parsed as an AMR verb, \textit{exhaust-01}, but JAMR is unable to recognize this because it has no instance of the adjective \textit{exhaustive} used in the training data. Our system can generalize from training data to learn that \textit{exhaustive} is to be treated as a verb, and match against the OntoNotes \needcite database of verb sense frames at test time to produce \textit{exhaust-01}.

% In order to train our system, we need a correspondence between AMR nodes and their source tokens in the training data. For example, a node with the title ``glee'' will almost certainly represent the token ``gleefully'' if that appears in the source text. However, AMR training data is ``unaligned'', meaning that no effort is made to annotate which token in a sentence is being represented by a given node in an AMR graph.

% Automatically inferring alignments is a source of noise in training our system.
% We also propose a novel alignment system inspired by our generative actions to explicitly minimize that noise, using a metric we define as ``action informativeness''.
% We cast AMR alignment as the task of finding the alignment of AMR graph to the source sentence that maximizes the informativeness of the implied generative actions.
% We define this further in \refsec{alignment}.

\Section{crash}{The AMR Formalism}

AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts.
The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment can often be generated it is not provided in the training corpora.
The semantics of nodes can represent lexical items (e.g., \w{dog}), sense tagged lexical items (e.g., \textit{run-01}), type markers (e.g., \textit{date-entity}), and a host of other phenomena.

The edges (relationships) in AMR describe one of a number of semantic relationships between concepts.
The most salient of these is semantic role labels, such as the \w{ARG0} and \w{destination} arcs in \reffig{method}.
However, often these arcs define a semantics more akin to syntactic dependencies (e.g., \textit{mod} standing in for adjective and adverbial modification), or take on domain-specific semantics (e.g., the month, day, and year arcs of a \textit{date-entity}).

AMR is based on neo-Davidsonian semantics, \cite{Davidson:1967,Parsons:1990}.
To introduce AMR and its notation in more detail, we'll unpack the translation of the sentence ``he gleefully ran to his dog Rover''. 
We show in \reffig{glee} the interpretation of this sentence as an AMR graph.

%AMR makes no effort to have a one-to-one correspondence between nodes in a graph and tokens in the sentence whose semantics is being represented.
%In fact, AMR will often expand single tokens into large sub-graph elements, or ignore tokens completely.
%It's important to understand that AMR represents the relationships between objects referred to by the surface text, \textit{not} the relationships between the words themselves.

% JON: Kind of inconvenient that Figure 2 is not on the same page as the explanation..
The root node of the graph is labeled \n{run-01}, corresponding to the PropBank \cite{palmer2005proposition-srl} definition of the verb \w{ran}.
%This is the name of a verb sense definition drawn from PropBank \needcite for the sense of the verb ``ran'' in this sentence.
\w{run-01} has an outgoing \e{ARG0} arc to a node \w{he}, with the usual PropBank semantics.
%semantics (drawn from the PropBank frame) that roughly correspond to ``he'' being the doer of the ``run-01'' action. 
The outgoing \e{mod} edge from \n{run-01} to \n{glee} takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent.
We note that \n{run-01} has a \e{destination} arc to \n{dog}.
The label for \e{destination} is taken from a finite set of special arc sense tags similar to the preposition senses found in \cite{srikumar2013-srl}.
The last portion of the figure parses \w{dog} to a node which will eventually serve as a type marker, and \w{Rover} into the larger subgraph indicating a concept with name ``Rover.''
%Then we have a section of the graph that is best interpreted as a unit, where all of the children of ``dog'' effectively mean that ``dog'' has the name ``Rover.''


%\Subsection{formal}{Formal task definition}
%Formally, an AMR graph is represented as a directed multigraph.
%
%Thus, we treat AMR as a directed graph $G$ of nodes $\bN = \{n_0 \dots n_k\}$, and
%  an edge matrix $\bA$ such that $\bA_{i,j} = l$ means that an arc exists from $n_i$ to $n_j$ with label $l$;
%  $\bA_{i,j} = \e{NONE}$ entails that no arc exists between the nodes.
%  $\bA$ is over a label space $\bL$: $\bA \in \bL^{k \times k}$.
%Each label $l \in \bL$ is one of the valid arc labels between two nodes (e.g.,
%  \e{ARG0}, \e{mod}), in addition to a special label denoting the lack of an arc, \e{NONE}.

%Formally, given an array of tokens $S = [s_0, \ldots, s_n]$, we generate a directed AMR graph $G$, defined as the pair $(N = [n_0, \ldots, n_k], A \in L^{k*k})$, where $N$ is an array of AMR nodes (which doesn't have to be the same length or have a clear correspondence to $S$), and $A$ is a matrix of labels $L$, where $A_{i,j} = l \in L$ means that an arc exists from node $n_i$ to node $n_j$ with label $l$ in the parsed graph. We include the special label ``NONE'' in $L$, corresponding to no arc existing between two nodes.

\Subsection{sub-chunks}{AMR Subgraphs}
% One-to-many mapping
The mapping from tokens of a sentence to AMR nodes is not one-to-one.
A single token or span of tokens can generate a \textit{subgraph} of AMR consisting
  of multiple nodes.
%AMR contains components that, while they may be composed of multiple nodes, 
These subgraphs can logically be considered the expression of a single concept,
  and are useful to treat as such (e.g., see \refsec{ner}).
%For the NER++ task, we would like to be able to generate these single concept subgraphs directly from spans of text.

% Figure: Sailors 
\Fig{sailor.png}{0.25}{sailor}{AMR representation of the word \w{sailor}, which is notable for breaking the word up into a self-contained multi-node unit unpacking the derivational morphology of the word.}

% Figure: Time expression 
\Fig{date.png}{0.25}{date}{AMR representation of the span \w{January 1, 2008}, an example of how AMR can represent structured data by creating additional nodes like \n{date-entity} to signify the presence of special structure}

% Structured data
Many of these multi-node subgraphs capture structured data such as time expressions
  , as in \reffig{date}.
%AMR can also capture structured data, like time expressions, see \reffig{date}. 
In this example, a \n{date-entity} node is created to signify that this cluster of 
  nodes is part of a structured sub-component representing a date, where the nodes
  and arcs within the component have specific semantics.
This illustrates a broader recurring pattern in AMR: an artificial node may, based on its title, have expected children with special semantics.
A particularly salient example of this pattern is the \n{name} node (see \n{``Rover''} in \reffig{glee}) which signifies 
  that all outgoing arcs with label \e{op} comprise the tokens of a name object.

% Hard cases
The ability to decouple the meaning representation of a lexical item from its
  surface form allows for rich semantic interpretations of certain concepts
  in a sentence.
%AMR makes an attempt to capture some semantic meanings in words that are 
%  difficult to capture in a way that is not domain specific. 
% JON: reword "difficult to capture in a more difficult to capture in a more principled way than dictionary lookups"
% JON: plus, what do "dictionary lookups" look like?
For example, the token \w{sailor} is represented in \reffig{sailor} by a concept graph 
  representing a person who performs the action \n{sail-01}. 
Whereas often the AMR node aligned to a span of text is a straightforward function
  of the text, these cases remain difficult to capture in a more principled way than
  simply memorizing mappings between tokens and sub-graphs.
%This is difficult to model without resorting to memorization, because the 
%  etymological clues are so sparse. 
%We note this as an area for further exploration.


\section{Task decomposition}


% Two stages

To the best of our knowledge, the JAMR parser is
the only published end-to-end AMR parser.
An important insight in JAMR is that AMR parsing can be broken into two 
relatively distinct tasks: (1) \textbf{NER++}: the task of interpreting what entities are being referred to in 
the text, realized by generating the best AMR sub-graphs for a given set of tokens, and
(2) \textbf{SRL++}: the task of discovering what 
relationships exist between entities, realized by taking the disjoint subgraphs generated
  by NER++ and creating a fully-connected graph.
We describe both tasks in more detail below.
%We define the terms \textbf{NER++} and \textbf{SRL++} in more detail within this section.

% +++
% We adopt the dual decomposition based MST algorithm from the parser, 
%   and contribute primarily towards replacing
%  the NER++ component.
%---

\Subsection{ner}{NER++}
% JON: s/To illustrate/For example/ ?
Much of the difficulty of parsing to AMR lies in generating local sub-graphs representing the meaning of token spans.
For instance, the formalism implicitly demands rich notions of NER, lemmatization, word sense disambiguation, number normalization, and temporal parsing; among others.
To illustrate, \reffig{method} requires lemmatization (\textit{gleefully} $\rightarrow$ \textit{glee}), word sense tagging (\textit{run} $\rightarrow$ \textit{run-01}), and open domain NER (i.e., \textit{Rover}),
%\reffig{date} shows an example of temporal parsing.
Furthermore, many of the generated subgraphs (e.g., \textit{sailor} in \reffig{sailor}) have rich semantics beyond those produced by standard NLP systems.

% JON: This is not formal compared to the formal-ness of previous sections (e.g. definition of A)
% We refer to this task as NER++ (see \reffig{method}).
Formally, NER++ is the task of generating a disjoint set of subgraphs representing the meanings of localized spans of words in the sentence.
% NOTE: not quite true, SRL++ uses dependency path features that rely on alignments
% This is also the task where AMR alignment is useful
% Whereas the structure of the AMR graphs themselves provide data for training the SRL++ model, the NER++ model requires an alignment between nodes of the AMR graph and the surface form of the sentence.
For NER++, JAMR uses a simple Viterbi sequence model to directly generate AMR-subgraphs from memorized mappings of text spans to subgraphs. The main contribution of this paper is our exploration of delexicalizing NER++ in \refsec{nerplusplus}.

%Parsing to the AMR representation demands a rich NER system, word sense disambiguation, number normalization, time parsing, and many semantic nominalizations and part of speech translations, and within-sentence coreference. We refer to these ``low-level'' AMR tasks collectively as NER++. NER++ is the sub-task of generating the best AMR sub-graphs (``sub-graph'' is defined in \refsec{sub-chunks}) given the set of tokens $S$. This involves both partitioning the source text into spans that will be rendered as a single sub-graph in AMR (e.g. ``run'', ``People's Republic of China'', ``January 1, 2008''), and then mapping each of those spans into a corresponding AMR sub-graph of maximum likelihood.

\Subsection{srl}{SRL++}
The second stage of the AMR decomposition consists of generating a coherent graph
  from the set of disjoint sub-graphs produced by NER++.
Unlike the domain-specific arcs within a single sub-graph produced by NER++, the
  arcs in SRL++ tend to have generally applicable semantics.
For example, the SRL arcs (e.g., \e{ARG0} and \e{destination} in \reffig{method}),
  or the syntactic dependency arcs (e.g., \e{mod} and \e{poss} in \reffig{method}).
For SRL++, JAMR uses a variation of the maximum spanning tree algorithm augmented by dual decomposition to impose linguistically motivated constraints on a maximum likelihood stitching. 
%JAMR's SRL++ component is extremely effective, and we were unable to produce a better SRL++ system in our experiments with several other structured prediction approaches.
%Therefore, we use the modified MST algorithm implemented in \newcite{2014flanigan-amr} to produce
%  a labeled DAG corresponding to the relation edges.

%Given a perfect NER++ system for an AMR parser, there remains the task of noting the verb arguments, preposition sense actionging, and doing some augmented semantic dependency parsing in order to join the disjoint NER++ output into a single AMR parse. We call this task SRL++. SRL++ is the sub-task of taking as input the disjoint sub-graphs generated by NER++, and adding the maximum likelihood set of arcs between the sub-graphs in order to have a fully connected graph.


\section{A Novel NER++ Method}\label{sec:nerplusplus}
38\% of the words in the LDC2014E113 dev set are 
  unseen during training time, highlighting the brittleness of memorization-based approaches.
We de-lexicalize by partitioning the AMR sub-graph search space in terms of the actions needed to 
  derive a node from its aligned token. 
At test time we do a sequence labeling of input tokens with these actions, and 
  then deterministically derive the AMR sub-graphs from spans of tokens by applying 
  the transformation decreed by their actions. 
We explain in \refsec{actions} how exactly we manage this partition, and in \refsec{data} how we create training data from existing resources to setup and train an action-type classifier.

\Subsection{actions}{Derivation actions}

We partition the AMR sub-graph space into a set of 9 actions, each corresponding to an action that will be taken by the NER++ system if a token receives this classification.

\paragraph{VERB} This action capture the verb-sense disambiguation feature of AMR. To execute on a token, we look for the most similar PropBank frame and make that the title of the corresponding node.

\paragraph{IDENTITY} This action handles the common case that the title of the node corresponding to a token is identical to the source token. To execute, we take the lowercased version of the token to be the title of the corresponding node.

\paragraph{VALUE} This action interprets a token by its integer value. 
The AMR representation is sensitive to the difference between a node with a title
  of \n{5} (the integer value) and ``5'' or ``five'' -- the string value.
This is a rare action, but is nonetheless distinct from any of the other classes.
%In AMR a node with title ``five'' and a node with title ``5'' are two distinct entities, with different semantic interpretation. 
We execute this action by extracting an integer value with a regex based number normalizer, and using that as the title of the generated node.

\paragraph{LEMMA} AMR often performs stemming and POS transformations on the source token in generating a node. 
For example, we get \n{glee} from \w{gleefully}.
We capture this by a \textbf{LEMMA} action, which is executed by using the lemma of the source token as the generated node title.
Note that this does not capture all lemmatizations, as there are often discrepancies
  between the lemma generated by the lemmatizer and the correct AMR lemma.

\paragraph{NONE} This action corresponds to ignoring this token, in the case that
  the node should not align to any corresponding AMR fragment.
%37\% of the time, we can just ignore the token.

\paragraph{NAME} A common structured data type in AMR is the \n{name} construction. 
For example, \w{Rover} in \reffig{glee}.
We can capture this phenomenon on unseen names by attaching a created \n{name} node to the top of a span.

\paragraph{PERSON} A related common structured data type in AMR is the \n{person} with \n{name} construction. 
For unknown people during broad-domain use, it's useful to have this classification category.
This action applies \textbf{NAME}, and then attaches a created \n{person} type-indicator above the created \n{name} for the span.

\paragraph{DATE} The most frequent of the structured data type in the data, after \n{name}, is the \n{date-entity} (for an example see \reffig{date}).
We deterministically take the output of SUTime \cite{2012chang-temporal}
  and convert it into the \n{date-entity} AMR representation
%  , and we integrate them using this tag.
%This action takes the output of the \textit{SUTime} system over the input span, and converts deterministically to a \n{date-entity} sub-graph representation.

\paragraph{DICT} This class serves as a backoff for the other classes, implementing
an approach similar to \newcite{2014flanigan-amr}.
%This is basically our catch-all.
In particular, we memorize a simple mapping from spans of text
  (like \w{sailor}) to their corresponding most frequently aligned AMR sub-graphs 
  in the training data (i.e., the graph in \reffig{sailor}). 
See \refsec{alignment} for details on the alignment process.
At test time we can do a lookup in this dictionary for any element that gets 
  labeled with a \textbf{DICT} action. 
If an entry is not found in the mapping, we back off to the second most probable
  class proposed by the classifier.
%Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. 
%\reftab{distro} gives the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).


%
% ACTION INFORMATIVENESS
%
\Subsection{informativeness}{Action Reliability}

\Fig{informativeness.png}{0.25}{hierarchy}{
Reliability of each action.
The top row are actions which are deterministic;
  the second row occasionally produce errors.
DICT is the least preferred action, with a relatively high error rate.
}

In this section we introduce a method for resolving ambiguity based on comparing the reliability with which actions generate the correct sub-graph, and discuss implications.

% all else being equal, we
  % prefer to generate actions from higher in the hierarchy, as they are more likely
  % to produce the correct sub-graph.

% Introduce informativeness
Even given a perfect action classification for a token, some action executions can introduce errors.
Some of our actions are entirely deterministic in their conversion
  from the word to the AMR sub-graph (e.g., IDENTITY), but others are prone to
  making mistakes in this conversion (e.g., VERB, DICT).
%Given that some span of tokens is correctly labeled with an action, 
We define the 
  notion of \textit{action reliability} as the probability of deriving the correct node from a span of tokens.

To provide a concrete example, our dictionary lookup classifier predicts the correct
  AMR sub-graph 67\% of the time on the dev set.
We therefore define the reliability of the \textbf{DICT} action as 0.67.
%, because given that we correctly label a token as \textbf{DICT}, there is a probability of 0.67 that we correctly generate the corresponding node.
In contrast to \textbf{DICT}, correctly labeling a node as \textbf{IDENTITY}, \textbf{NAME}, \textbf{PERSON}, and \textbf{NONE} have 
  action reliability of 1.0, since there is no ambiguity in the node generation 
  once one of those actions have been selected, 
  and we are guaranteed to generate the correct node given the correct action.

We can therefore construct a hierarchy of reliability (\reffig{hierarchy}) -- all else being equal, we
  prefer to generate actions from higher in the hierarchy, as they are more likely
  to produce the correct sub-graph.
This hierarchy is useful in resolving ambiguity throughout our system. 
During data generation for training (\refsec{data}), when two actions could both generate the aligned AMR node, we prefer the more reliable one.
In our aligner, we bias alignments towards generating more reliable action sequences (see \refsec{alignment}).

%This allows us to induce an action informativeness hierarchy, with more informative actions taking precedence over less informative actions for several important tasks. We demonstrate this hierarchy in \reffig{hierarchy}.

The primary benefit of the de-lexicalized NER++ approach is that we can reduce the usage of low reliability actions, like \textbf{DICT}. A fully-lexicalized NER++ system is equivalent to using \textbf{DICT} for everything.

%
% ACTION DISTRIBUTION
%
% \Subsection{actions}{Distribution of Actions}

% Distribution of Actions
\begin{table}[t]
\begin{center}
\begin{tabular}{l|rr}
\bf Action & \bf \# Tokens & \bf \% Total \\ \hline
NONE & 41538 & 36.2 \\
DICT & 30027 & 26.1 \\
IDENTITY & 19034 & 16.6 \\
VERB & 11739 & 10.2 \\
LEMMA & 5029 & 4.5 \\
NAME & 4537 & 3.9 \\
DATE & 1418 & 1.1 \\
PERSON & 1336 & 1.1 \\
VALUE & 122  & 0.1\\
\end{tabular}
\end{center}
\caption{\label{tab:distro} Distribution of action types in the proxy section of the newswire section of the LDC2014T12 dataset, generated from automatically aligned data. }
\end{table}

We analyze the empirical distribution of actions in our automatically aligned corpus in \reftab{distro}.
The cumulative frequency of the non-\textbf{DICT} actions is striking: we can generate 74\% of the tokens with high reliability ($p \geq 0.95$) actions, which will not break on unseen words.
That means we can handle 74\% of the words in the dev set that are unobserved during training, leaving only 9\% of the total to brittle unobserved memorization, as opposed to the 38\% prior to our actions.
In this light, it is unsurprising that our results demonstrate a large gain in recall on the test set.

%We believe that \textbf{DICT} should count for much less than 27\%, and \textbf{LEMMA} should count for much more than 4\%, but issues with existing lemmatizers prevent this, see our error analysis in \refsec{errors}.


%It's not always possible to derive an AMR sub-graph directly from tokens at test 
%  time without having memorized a mapping. 
%For example, the parse of \w{sailor} as ``person who sails'' (\reffig{sailor}),
%  is nearly impossible without some form of memorization. 
%That's where the \textbf{DICT} class is important.

%To implement a \textbf{DICT} class, we memorize a simple mapping from spans of text, like ``sailor'' to their corresponding most frequently seen AMR sub-graphs in the training data, in this case \reffig{sailor}. At test time we can do a lookup in this dictionary for any element that gets labeled with a \textbf{DICT} action. Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. \reftab{distro} gives the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).


%Note that \textbf{DICT} counts for around 27\% of the training data, meaning that more than 72\% of tokens can be generated correctly by our action type classifier even if we've never seen them before, which is a huge win.

%
% TRAINING
%
\Subsection{data}{Training the Action Classifier}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{l}
Input token; word embedding                  \\
Left+right token / bigram                    \\
Token length indicator     \\
Token starts with ``non''     \\
POS; Left+right POS / bigram                 \\
Dependency parent token / POS                \\
Incoming dependency arc                      \\
Bag of outgoing dependency arcs              \\
Number of outgoing dependency arcs           \\
Max JaroWinkler to any lemma in PropBank      \\
Output tag of the \textbf{VERB} action if applied \\
Output tag of the \textbf{DICT} action if applied \\
NER; Left+right NER / bigram                 \\
Capitalization                               \\
Incoming prep\_* or appos + parent has NER   \\
Token is pronoun                             \\
Token is part of a coref chain               \\
Token pronoun and part of a coref chain     \\
\end{tabular}
\end{center}
\caption{\label{tab:features} The features for the NER++ maxent classifier. }
\end{table}

Given a set of AMR training data, in the form of (graph,sentence) pairs,
  we first induce alignments from the graph nodes to the sentence 
  (see \refsec{alignment}).
Formally, alignment gives us that for each node in the AMR graph $n_i$ we have a token $s_k$ in the
  sentence that we believe generated the node $n_i$.
%With this alignment, which is an annotation on the graph noting for each node $N_i$ 
%  a token $S_j$ that is most likely to have ``generated'' $N_i$, we can induce 
%  alignments.

Then for each action type, we can ask whether that action type is able to take 
  token $s_k$ and correctly generate $n_i$. 
For concreteness, imagine the token $s_k$ is \w{running}, and the node $n_i$ has 
  the title \n{run-01}.
%In our example,
The two action types we find that are 
  able to correctly generate this node are DICT and VERB. 
We choose the most reliable action type of those available (see \reffig{hierarchy})
  to generate the observed node -- in this case, VERB.
  
This provides a supervised training set for our action classifier, with some noise introduced by the automatically generated alignments.
We then train a simple maxent classifier to make action decisions at each node. 
At test time,
  the classifier takes as input a pair $\langle i, S \rangle$, where $i$ is the 
  index of the token in the input sentence, and $S$ is a sequence tokens 
  representing the source sentence.
It then uses the features in \reftab{features} to predict the actions to take at
  that node.

%At test time, given a sequence of input tokens, we do a simple classification of each token separately, to get a sequence labeling of our input tokens. Then for each token, we apply the behavior associated with the token label, and the resulting set of sub-graphs is passed on to SRL++ for linking.

%In general, our algorithm is as follows. For all $S_j$ to which no $N_i$ exists 
%  such that $N_i$ aligns to $S_j$, assign the action \textbf{NONE} to $S_j$.
%For all pairs $N_i$, $S_j$, assign $S_j$ the most informative action possible 
%  that could have generated $N_i$. \todo{Flesh out discussion of adjacent DICT nodes}
%
%\Subsection{classifier}{Action Classifier}
%
%The output of the classifier is an action $T$ such that the likelihood with respect to the data of token $i$ in sentence $S$ generating a node according to the action specified by $T$ is maximized. See Appendix A for a list of classifier features.
%
%\todo{How do you work out verb senses?}
%
%\subsection{Test Time Behavior}


\input alignments.tex


\Section{related}{Related Work}

Beyond the connection of our work with \newcite{2014flanigan-amr}, we note that 
the NER++ component of AMR encapsulates a number of lexical NLP tasks.
These include named entity recognition \cite{2007nadeau-ner,stanford-ner},
  word sense disambiguation \cite{1995yarowsky-wsd,2002banerjee-wsd},
  lemmatization, and a number of more domain specific tasks.
For example, a full understanding of AMR requires normalizing temporal
  expressions \cite{2010verhagen-tempeval,2010strotgen-temporal,2012chang-temporal}.
 
In turn, the SRL++ facet of AMR takes many insights from semantic role labeling
  \cite{2002gildea-srl,2004punyakanok-srl,srikumar2013-srl} to capture the
  relations between verbs and their arguments.
In addition, many of the arcs in AMR have nearly syntactic interpretations
  (e.g., \e{mod} for adjective/adverb modification, \e{op} for compound noun
  expressions).
These are similar to representations used in syntactic dependency parsing
  \cite{stanford-dependencies,2005mcdonald-dependency0,2006buchholz-conll}.

More generally, parsing to a semantic representation is has been explored in
  depth for when the representation is a logical form
  \cite{2005kate-semantics,2005zettlemoyer-semantics,2011liang-semantics}.
Recent work has applied semantic parsing techniques to representations beyond
  lambda calculus expressions.
For example, work by \newcite{2014berant-bio} parses
  text into a formal representation of a biological process.
\newcite{2014hosseini-algebra} solves algebraic word problems by parsing them
  into a structured meaning representation.
In contrast to these approaches, AMR attempts to capture open domain semantics
  over arbitrary text.

Interlingua
  \cite{1991mitamura-interlingua,1999carbonell-interlingua,1998levin-interlingua}
  are an important inspiration for decoupling the semantics of the AMR language
  from the surface form of the text being parsed; although, AMR has a self-admitted
  English bias.




\section{Results}
We present improvements in end-to-end AMR parsing on two datasets by using our NER++ component.
Action type classifier accuracy on an automatically aligned corpus
and alignment accuracy on a small hand-labeled corpus are also reported.

\Subsection{result-amr}{End-to-end AMR Parsing}
We evaluate our NER++ component in the context of end-to-end AMR parsing
on two corpora: the newswire section of LDC2014T12 and the split given in \cite{2014flanigan-amr} of LDC2013E117, both consisting primarily of newswire.
We compare two systems: the JAMR parser \cite{2014flanigan-amr}
\footnote{Available at \url{https://github.com/jflanigan/jamr}.},
  and the JAMR SRL++ component with our NER++ approach.

AMR parsing accuracy is measured with a metric called \w{smatch} \cite{cai2013smatch-amr}, which stands 
  for ``s(emantic) match.'' 
The metric is the F$_1$ of a best-match between triples implied by the target graph, 
  and triples in the parsed graph. 

Our results are given in \reftab{results}.
We report much higher recall numbers on both datasets, with only small ($\leq 1$ point) 
  loss in precision.
This is natural considering our approach.
A better NER++ system allows for more correct AMR sub-graphs to be generated --
  improving recall -- but does not in itself improve the accuracy of the
  SRL++ system it is integrated in.

%Our end-to-end results are reported by plugging the output of our NER++ into the SRL++ component of JAMR \cite{2014flanigan-amr},\footnote
%  {Available at \url{https://github.com/jflanigan/jamr}.}
%which is able to produce final AMR graphs when given a sequence of spans and their corresponding chunks. 

\begin{table}[t]
\begin{center}
\begin{tabular}{l|l|llr}
\textbf{Dataset} &  \textbf{System} & \textbf{P} & \textbf{R} & \textbf{F$_1$} \\
\hline
\multirow{2}{*}{2014T12} & JAMR & \textbf{67.1} & 53.2 & 59.3 \\
  & \textbf{Our System} & 66.6 & \textbf{58.3} & \textbf{62.2} \\
\hline
\multirow{2}{*}{2013E117} & JAMR & \textbf{66.9} & 52.9 & 59.1 \\
  & \textbf{Our System} & 65.9 & \textbf{59.0} & \textbf{62.3} \\
\end{tabular}
\end{center}
\caption{\label{tab:results} 
Results on two AMR datasets for JAMR and our NER++ embedded in the JAMR SRL++
  component.
Note that recall is consistently higher across both datasets, with only a small
  loss in precision.
}
\end{table}

\Subsection{result-component}{Component Accuracy}
We evaluate our aligner on a small set of 100 hand-labeled alignments,
  and evaluate our NER++ classifier on automatically generated alignments over the whole corpus,
  
On a hand-annotated dataset of 100 AMR parses from the LDC2014T12 corpus,\footnote{
    This corpus would be publicly released if the paper is accepted.
  }
  our aligner achieves
  an accuracy of \textbf{83.2}.
This is a measurement of the percentage of AMR nodes that are
  aligned to the correct token in their source sentence.
Note that this is a different metric than the precision/recall of prior work, and
  is based on both a different corpus and subtly different annotation scheme.\footnote{ 
  A standard semantics and annotation guideline for AMR alignment is left for 
}
  future work; our accuracy should be considered only an informal metric.
In particular, we require that every AMR node aligns to some token in the sentence,
  and do not allow the system to not align unsure nodes.

%Previous aligners have pursued a slightly different definition of alignment,
%  and so are not strictly comparable, though we achieve good precision given that we force every node
%  to align to a token.


On the automatic alignments over the LDC2014T12 corpus,
  our action classifier achieved a test accuracy of \textbf{0.841}.
The \textbf{DICT} action lookup table achieved an accuracy of \textbf{0.67}.
This is particularly impressive given that our model moves many of the difficult 
  semantic tasks onto the \textbf{DICT} tag, and that this lookup does not make
  use of any learning beyond a simple count of observed span to sub-graph mappings.

% Our approach really shines in domain transfer. Using a system trained on \textbf{LDC2013E117}, which is composed of international newswire, and testing on the web forum subsection \textbf{LDC2014T12}, our system generalizes well.

\Section{conclude}{Conclusion}
We address a key challenge in AMR parsing: 
  the task of generating subgraphs from lexical items in the sentence.
We show that a 
  simple classifier over \textit{actions} which generate
  these graphs improves end-to-end recall for AMR parsing with only a small
  drop in precision, leading to an overall gain in F$_1$.
A clear direction of future work is improving the coverage of the defined actions.
For example, a richer lemmatizer could shift the burden of lemmatizing unknown
  words into the AMR lemma semantics away from the dictionary lookup component.
  
%Future work on improving the accuracy of the graph generation from
%  actions -- particularly the lemmatizer -- and on further reducing the number of
%  tokens which must be generated by a dictionary lookup.

%\section{Future Work}
%
%Many features of NER++ are already semantic parsing tasks in their own right and need just to be integrated in a coherent way. One feature in particular stands out as not having ready-made systems available: a derivational morphology system for parsing words like `sailor' that would benefit AMR parsing domain generalization tremendously. We envision a system that upon discovering the unseen noun `arbitrageur' at training time is able to retrieve the known verb root `arbitrage', and to derive that the noun `arbitrageur' probably refers to an entity that is engaged in \n{arbitrage-01}. The AMR corpus provides an opportunity to perform and measure such a task.

% \section{Appendix}

% \section*{Acknowledgments}

% The acknowledgments should go immediately before the references.  Do
% not number the acknowledgments section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}

\end{document}

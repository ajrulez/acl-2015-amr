%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathrsfs}

\input std-macros.tex

\newcommand\w[1]{\textit{#1}} % A textual phrase
\newcommand\e[1]{\textit{#1}} % An AMR edge
\newcommand\n[1]{\textit{#1}} % An AMR node

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Robust Subgraph Generation Improves Abstract Meaning Representation Parsing}
\author{
%  Keenon Werling \\
%  Stanford University\\
%  {\tt keenon@stanford.edu} \\\And
%  Gabor Angeli \\
%  Stanford University\\
%  {\tt angeli@cs.stanford.edu} \\\And
%  Chris Manning \\
%  Stanford University\\
%  {\tt manning@cs.stanford.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

% The existing AMR corpus is heavily biased to the international newswire domain, making domain transfer of AMR parsing an important challenge. 
% The Abstract Meaning Representation (AMR) is a semantic formalism for which a large and growing set of annotated examples is available.

Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like semantic parsing
  and machine translation.
We identify that node generation, typically done using a simple
  dictionary lookup, is currently the crucial limiting factor in AMR parsing.
% The state-of-the-art AMR parser cannot generate nodes for tokens unseen at training time, limiting its generalization.
We propose a small set of actions to construct parts of the AMR parse from spans of tokens, which allows for more robust learning of this stage.
%the first AMR parser that can handle words at test time that were unseen during training.
We show that our set of construction actions generalize better than the previous approach, even when learned with an extremely simple classifier.
% These actions also provide an insight for an alignment system that yields a ``maximally informative'' set of action labels, which we show yields good results.
We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end F1 from 0.59 to 0.62 on the LDC2013E117 and LDC2014T12 datasets.

\end{abstract}

\section{Introduction}

\Fig{glee.png}{0.25}{glee}{The AMR graph for ``He gleefully ran to his dog Rover''. Nodes represent concepts, and arcs are relationships between concepts. The dark arc labeled ``op1'' is expected to be generated by NER++.}

Abstract Meaning Representation (AMR) \cite{2013banarescu-amr} is a rich graph-based language for expressing semantics over a broad domain.
% AMR is written as a directed, rooted graph merging propositional logic and neo-Davidsonian semantics (\todo{cite}).
\reffig{glee} shows an example AMR for ``he gleefully ran to his dog Rover'', and we give a brief tutorial on AMR in \refsec{crash}.

AMR parsing is the task of mapping a natural language sentence into an AMR graph.
AMR parsing is exciting because a practical broad-domain AMR parser could enable a new breed of natural language applications ranging from semantically aware MT to rich broad-domain QA over text-based knowledge bases.
At the time of this writing AMR is the target of a multi-institution multi-year data-labeling program led by Kevin Knight, which promises to produce a corpus that is large enough to make it possible to parse to AMR well despite its many challenges.

\FigStar{method.png}{0.23}{method}{Derivation process for ``He gleefully ran to his dog Rover''. First the tokens in the sentence are labeled with derivation actions, then those actions are used to generate AMR sub-graphs, and then those sub-graphs are stitched together to form a coherent whole.}

However, no matter how much data we are soon to have, AMR parsing remains a hard unsolved task, with only modest performance numbers reported so far.
A good AMR parser needs to be able to handle a broad range of semantic interpretation tasks, and 
AMR also possesses distinctive computational challenges from structural properties such as providing no guarantees about projectivity or even acyclicity of its graphs.

We follow previous work \cite{2014flanigan-amr} in dividing AMR parsing into two steps. 
The first step is \textit{concept identification}, which generates concept (or entity) nodes from text, and which we'll refer to as \textit{NER++} (\refsec{ner}). 
The second step is \textit{relation identification}, which adds arcs to link these nodes into a fully connected AMR graph, which we'll call \textit{SRL++} (\refsec{srl}).

Our initial assumption (which we suspect would be shared by many researchers with a background in structured prediction) was to assume that the main area deserving further attention in AMR parsing is predicting the edges in the not well-aligned, cyclic non-projective graphs of AMR (i.e., SRL++).
This intuition follows by analogy from the locus of work for syntactic dependency parsing. We spent months testing novel structure-prediction algorithms and training regimes to capture aspects of the non-projective cyclic graphs, only to find that the careful application of MST presented in \newcite{2014flanigan-amr} set an extremely strong baseline for the SRL++ task, and our gains were negligible to non-existent.

Over time we realized that SRL++ is \textit{not} the hard part of AMR parsing. The hard part of AMR parsing is NER++. A first piece of evidence for this claim comes from a close analysis of the only published AMR parser to date, \cite{2014flanigan-amr}, dubbed ``JAMR''.
When JAMR parses using its own NER++ and SRL++ components, it gets an end-to-end score of 0.58 F1.
However, when JAMR is given a gold NER++ output, and must only perform SRL++ over given sub-graphs it scores 0.80 F1 against a virtual upper bound of 0.83 F1, which is inter-annotator agreement on AMR \todo{NEED CITE FOR THIS}.  (And annotators very rarely disagree on NER++, which amounts to disagreeing about the entities mentioned in a sentence). \todo{verify -- CDM: I'm not sure this is true} [since surely there is quite a bit of inconsistency in NER++, such as for which compound words with derivational morphology to expand into subgraphs.]
So this means the SRL++ component is nearly perfect, if given perfect NER++.

In retrospect, it makes sense that SRL++ within AMR is \textit{relatively} easy given a perfect set of nodes to link together.
There's a strong type-check feature for the existence and type of any arc just by looking at its end-points, and syntactic dependency features are very informative for removing any remaining ambiguity.
If a system is considering how to link the node ``run-01'' in \reffig{glee}, the verb-sense frame for ``run-01'' leaves very little entropy in terms of what we could assign as an ARG0 arc.
It must be a noun, which leaves either ``he'' or ``dog'', and this is easily decided in favor of ``he'' by looking for an nsubj arc in the dependency parse. Given a modest amount of data, a simple parser can learn these lexical and syntactic type-check rules.

While SRL++ is hard, NER++ is extremely challenging, encompassing many SemEval tasks in a single monolith.
This task was handled by a simple baseline system in \cite{2014flanigan-amr}, which memorized a purely lexical mapping from spans of text to AMR nodes, augmented by an NER system and time expression regex at test time.
This is problematic, because fully 38\% of the tokens in the test set are unobserved at training time, and the existing method has no way to handle generation for them.

The primary contribution of this paper is a method to largely delexicalize NER++ to gracefully handle unseen tokens.
Choosing from among a small set of `generative actions' our system can derive an AMR sub-graph from a span of tokens (see \reffig{method}).
For example, we have an action \textit{VERB} that will perform a verb-sense-disambiguation to the appropriate PropBank frame \cite on the source token, like the ``ran'' to ``run-01'' example in \reffig{glee}.

We show that this approach improves recall dramatically over previous approaches, and that end to end performance is improved from 0.59 to 0.62 smatch when our generative actions are stitched together by the previous state of the art parser \cite{2014flanigan-amr}.
We suspect that the gains are not even bigger mainly because our NER++ system is able to generate nodes that the SRL++ system has never seen during training time, which makes the still lexicalized typecheck features of the SRL++ component useless.

% Previous work has observed that AMR parsing can be partitioned into two tasks: a rich lexically grounded entity detection system, which we call NER++ (see \refsec{ner}), and a relationship detection system, which we call SRL++ (see \refsec{srl}).

% As an example of the distinction, let's briefly go over \reffig{glee}, the AMR for ``he gleefully ran to his dog Rover''. To produce this parse, the NER++ task will have to do a verb sense disambiguation on ``ran'' to get ``run-01'', and a lemmatization on ``gleefully'' to ``glee''. Then NER++ will have to recognize ``Rover'' is a name, and generate the sub-graph (name :op1 ``Rover''). It will also have to recognize that ``to'' is a dropped preposition, and so doesn't get a node, and ``his'' is a coreferant that also doesn't get a node. Then ``dog'' and ``he'' will have to be generated directly as nodes. The SRL++ task is then responsible for linking together the output of the NER++ task to produce a fully-connected semantic graph.

% At test time, AMR parsing is commonly divided into two tasks, which we refer to as NER++ and SRL++.

% Valid examples to use:
% excessively -> excessive
% mother -> mother
% exhaustive -> exhaust-01

% For instance, the adjective \textit{exhaustive} in ``the \textit{exhaustive} search'' should be parsed as an AMR verb, \textit{exhaust-01}, but JAMR is unable to recognize this because it has no instance of the adjective \textit{exhaustive} used in the training data. Our system can generalize from training data to learn that \textit{exhaustive} is to be treated as a verb, and match against the OntoNotes \needcite database of verb sense frames at test time to produce \textit{exhaust-01}.

% In order to train our system, we need a correspondence between AMR nodes and their source tokens in the training data. For example, a node with the title ``glee'' will almost certainly represent the token ``gleefully'' if that appears in the source text. However, AMR training data is ``unaligned'', meaning that no effort is made to annotate which token in a sentence is being represented by a given node in an AMR graph.

% Automatically inferring alignments is a source of noise in training our system.
% We also propose a novel alignment system inspired by our generative actions to explicitly minimize that noise, using a metric we define as ``action informativeness''.
% We cast AMR alignment as the task of finding the alignment of AMR graph to the source sentence that maximizes the informativeness of the implied generative actions.
% We define this further in \refsec{alignment}.

\Section{crash}{A Crash-Course in AMR}

AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts.
The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment can often be generated it is not provided in the training corpora.
The semantics of nodes can represent lexical items (e.g., \w{dog}), sense tagged lexical items (e.g., \textit{run-01}), type markers (e.g., \textit{date-entity}), and a host of other phenomena.

The edges (relationships) in AMR describe one of a number of semantic relationships between concepts.
The most salient of these is semantic role labels, such as the ARG0 and destination arcs in \reffig{method}.
However, often these arcs define a semantics more akin to syntactic dependencies (e.g., \textit{mod} standing in for adjective and adverbial modification), or take on domain-specific semantics (e.g., the month, day, and year arcs of a \textit{date-entity}).

AMR is based on neo-Davidsonian semantics, \cite{Davidson:1967,Parsons:1990}.
To introduce AMR and its notation in more detail, we'll unpack the translation of the sentence ``he gleefully ran to his dog Rover''. 
We show in \reffig{glee} the interpretation of this sentence as an AMR graph.

%AMR makes no effort to have a one-to-one correspondence between nodes in a graph and tokens in the sentence whose semantics is being represented.
%In fact, AMR will often expand single tokens into large sub-graph elements, or ignore tokens completely.
%It's important to understand that AMR represents the relationships between objects referred to by the surface text, \textit{not} the relationships between the words themselves.

% JON: Kind of inconvenient that Figure 2 is not on the same page as the explanation..
The root node of the graph is labeled \n{run-01}, corresponding to the PropBank \cite{Palmer:2005} definition of the verb \w{ran} .
%This is the name of a verb sense definition drawn from PropBank \needcite for the sense of the verb ``ran'' in this sentence.
\w{run-01} has an outgoing \e{ARG0} arc to a node \w{he}, with the usual PropBank semantics.
%semantics (drawn from the PropBank frame) that roughly correspond to ``he'' being the doer of the ``run-01'' action. 
The outgoing \e{mod} edge from \n{run-01} to \n{glee} takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent.
We note that \n{run-01} has a \e{destination} arc to \n{dog}.
The label for \e{destination} is taken from a finite set of special arc sense tags similar to the preposition senses found in \cite{Srikumar:2013}.
The last portion of the figure parses \w{dog} to a node which will eventually serve as a type marker, and \w{Rover} into the larger subgraph indicating a concept with name ``Rover.''
%Then we have a section of the graph that is best interpreted as a unit, where all of the children of ``dog'' effectively mean that ``dog'' has the name ``Rover.''


\Subsection{formal}{Formal task definition}
Formally, an AMR graph is represented as a directed multigraph.
Following \newcite{2014flanigan-amr}, we simplify the representation by (1) disallowing
  cycles in the graph, and (2) disallowing multiple edges between two nodes in a
  graph.
Both of these are rare phenomena in the representation, and substantially complicate
  the task.

Thus, we treat AMR as a directed graph $G$ of nodes $\bN = \{n_0 \dots n_k\}$, and
  an edge matrix $\bA$ such that $\bA_{i,j} = l$ means that an arc exists from $n_i$ to $n_j$ with label $l$;
  $\bA_{i,j} = \e{NONE}$ entails that no arc exists between the nodes.
  $\bA$ is over a label space $\bL$: $\bA \in \bL^{k \times k}$.
Each label $l \in \bL$ is one of the valid arc labels between two nodes (e.g.,
  \e{ARG0}, \e{mod}), in addition to a special label denoting the lack of an arc, \e{NONE}.

%Formally, given an array of tokens $S = [s_0, \ldots, s_n]$, we generate a directed AMR graph $G$, defined as the pair $(N = [n_0, \ldots, n_k], A \in L^{k*k})$, where $N$ is an array of AMR nodes (which doesn't have to be the same length or have a clear correspondence to $S$), and $A$ is a matrix of labels $L$, where $A_{i,j} = l \in L$ means that an arc exists from node $n_i$ to node $n_j$ with label $l$ in the parsed graph. We include the special label ``NONE'' in $L$, corresponding to no arc existing between two nodes.

\Subsection{sub-chunks}{AMR Subgraphs}
% One-to-many mapping
The mapping from tokens of a sentence to AMR nodes is not one-to-one.
A single token or span of tokens can generate a \textit{subgraph} of AMR consisting
  of multiple nodes.
%AMR contains components that, while they may be composed of multiple nodes, 
These subgraphs can logically be considered the expression of a single concept,
  and are useful to treat as such (e.g., see \refsec{ner}).
%For the NER++ task, we would like to be able to generate these single concept subgraphs directly from spans of text.

% Figure: Sailors 
\Fig{sailor.png}{0.25}{sailor}{AMR representation of the word \w{sailor}, which is notable for breaking the word up into a self-contained multi-node unit unpacking the derivational morphology of the word.}

% Figure: Time expression 
\Fig{date.png}{0.25}{date}{AMR representation of the span \w{January 1, 2008}, an example of how AMR can represent structured data by creating additional nodes like \n{date-entity} to signify the presence of special structure}

% Structured data
Many of these multi-node subgraphs capture structured data such as time expressions
  (\reffig{date}).
%AMR can also capture structured data, like time expressions, see \reffig{date}. 
In this example, a \n{date-entity} node is created to signify that this cluster of 
  nodes is part of a structured sub-component representing a date, where the nodes
  and arcs within the component have specific semantics.
This illustrates a broader recurring pattern in AMR, which is to have an 
  artificial node with certain expected children with special semantics.
A particularly salient example of this pattern is the \n{name} node (see ``Rover'' in \reffig{glee}) which signifies 
  that all outgoing arcs with label \e{op} comprise the tokens of a name object.

% Hard cases
The ability to decouple the meaning representation of a lexical item from its
  surface form allows for rich semantic interpretations of certain concepts
  in a sentence.
%AMR makes an attempt to capture some semantic meanings in words that are 
%  difficult to capture in a way that is not domain specific. 
% JON: reword "difficult to capture in a more difficult to capture in a more principled way than dictionary lookups"
% JON: plus, what do "dictionary lookups" look like?
For example, the token \w{sailor} is represented in \reffig{sailor} by a concept graph 
  representing a person who performs the action \n{sail-01}. 
Whereas often the AMR node aligned to a span of text is a straightforward function
  of the text, these cases remain difficult to capture in a more principled way than
  dictionary lookups.
%This is difficult to model without resorting to memorization, because the 
%  etymological clues are so sparse. 
%We note this as an area for further exploration.


\section{Task decomposition}


% Two stages

To the best of our knowledge, the JAMR parser \cite{2014flanigan-amr} is
the only published end-to-end AMR parser.
The crucial insight in JAMR is that AMR parsing can be broken into two 
relatively distinct tasks: (1) \textbf{NER++} -- the task of interpreting what entities are being referred to in 
the text, realized by generating the best AMR sub-graphs for a given set of tokens, and
(2) \textbf{SRL++} -- the task of discovering what 
relationships those entities have between one another other, realized by taking the disjoint subgraphs generated
  by NER++ and creating a fully-connected graph.
We describe both tasks in more detail below.
%We define the terms \textbf{NER++} and \textbf{SRL++} in more detail within this section.

% +++
% We adopt the dual decomposition based MST algorithm from the parser, 
%   and contribute primarily towards replacing
%  the NER++ component.
%---

\Subsection{ner}{NER++}
% JON: s/To illustrate/For example/ ?
Much of the difficulty of parsing to AMR lies in generating local sub-graphs representing the meaning of token spans.
For instance, the formalism implicitly demands rich notions of NER, lemmatization, word sense disambiguation, number normalization, and temporal parsing; among others.
To illustrate, \reffig{method} requires lemmatization (\textit{gleefully} $\rightarrow$ \textit{glee}), word sense tagging (\textit{run} $\rightarrow$ \textit{run-01}), and open domain NER (i.e., \textit{Rover}); \reffig{date} shows an example of temporal parsing.
Furthermore, many of the generated subgraphs (e.g., \textit{sailor} in \reffig{sailor}) have rich semantics beyond those produced by standard NLP systems.

% JON: This is not formal compared to the formal-ness of previous sections (e.g. definition of A)
% We refer to this task as NER++ (see \reffig{method}).
Formally, this is the task of generating a disjoint set of subgraphs representing the meanings of localized spans of words in the sentence.
% NOTE: not quite true, SRL++ uses dependency path features that rely on alignments
% This is also the task where AMR alignment is useful
% Whereas the structure of the AMR graphs themselves provide data for training the SRL++ model, the NER++ model requires an alignment between nodes of the AMR graph and the surface form of the sentence.
For NER++, JAMR uses a simple Viterbi sequence model to directly generate AMR-subgraphs from memorized mappings of text spans to subgraphs. The main contribution of this paper is our exploration of a better way of doing NER++ in \refsec{nerplusplus}.

%Parsing to the AMR representation demands a rich NER system, word sense disambiguation, number normalization, time parsing, and many semantic nominalizations and part of speech translations, and within-sentence coreference. We refer to these ``low-level'' AMR tasks collectively as NER++. NER++ is the sub-task of generating the best AMR sub-graphs (``sub-graph'' is defined in \refsec{sub-chunks}) given the set of tokens $S$. This involves both partitioning the source text into spans that will be rendered as a single sub-graph in AMR (e.g. ``run'', ``People's Republic of China'', ``January 1, 2008''), and then mapping each of those spans into a corresponding AMR sub-graph of maximum likelihood.

\Subsection{srl}{SRL++}
The second stage of the AMR decomposition consists of generating a coherent graph
  from the set of disjoint sub-graphs produced by NER++.
Unlike the domain-specific arcs within a single sub-graph produced by NER++, the
  arcs in SRL++ tend to have generally applicable semantics.
For example, the SRL arcs (e.g., \e{ARG0} and \e{destination} in \reffig{method}),
  or the syntactic dependency arcs (e.g., \e{mod} and \e{poss} in \reffig{method}).
For SRL++, JAMR uses a variation of the maximum spanning tree algorithm augmented by dual decomposition to impose linguistically motivated constraints on a maximum likelihood stitching. JAMR's SRL++ component is extremely effective, and we were unable to produce a better SRL++ system in our experiments with several other structured prediction approaches.
Therefore, we use the modified MST algorithm implemented in \newcite{2014flanigan-amr} to produce
  a labeled DAG corresponding to the relation edges.

%Given a perfect NER++ system for an AMR parser, there remains the task of noting the verb arguments, preposition sense actionging, and doing some augmented semantic dependency parsing in order to join the disjoint NER++ output into a single AMR parse. We call this task SRL++. SRL++ is the sub-task of taking as input the disjoint sub-graphs generated by NER++, and adding the maximum likelihood set of arcs between the sub-graphs in order to have a fully connected graph.


\section{A Novel NER++ Method}\label{sec:nerplusplus}

Our approach to improving NER++ is very simple: instead of trying to pick which of thousands of AMR sub-graphs to generate from a span of text directly, we partition the AMR sub-graph space in terms of the actions needed to derive a node from its aligned token. At test time we do a sequence labeling of input tokens with these actions, and then deterministically derive the AMR sub-graphs from spans of tokens by applying the transformation decreed by their actions. This dramatically reduces sparsity, and helps improve end-to-end performance, but is most beneficial for domain transfer. We explain in \refsec{actions} how exactly we manage this partition, and explain in \refsec{data} how we create training data from existing resources to train a action-type classifier. Then we setup the classifier itself in \refsec{classifier}.

\Subsection{actions}{Derivation actions}

We partition the AMR sub-graph space into a set of 7 actions, each corresponding to an action that will be taken by the NER++ system if a token receives this classification.

\begin{itemize}
\item \textbf{VERB}: Look for the most similar PropBank frame, make that the title of the corresponding node.
\item \textbf{IDENTITY}: Take the lowercased version of the token to be the title of the corresponding node.
\item \textbf{VALUE}: Parse the token to an integer value, and use that as the node. AMR actually does type-check, so 
\item \textbf{LEMMA}: Take the lemma of the token to be the title of the corresponding node.
\item \textbf{NONE}: Ignore this token in the final output.
\item \textbf{NAME}: Attach a created ``name'' node to the top of this span, but don't add an NER action type on top of the ``name'' node.
\item \textbf{DICT}: Look up the most probable chunk associate with this lexical span. This functions as a back off if no other actions are appropriate.
\end{itemize}

\Subsection{actions}{Notes on the DICT action}

It's not always possible to derive an AMR sub-graph directly from tokens at test time without having memorized a mapping. For example, the parse of ``sailor'' as ``person who sails'', see \reffig{sailor}, is nearly impossible without some form of memorization. That's where the \textbf{DICT} class is important.

To implement a \textbf{DICT} class, we memorize a simple mapping from spans of text, like ``sailor'' to their corresponding most frequently seen AMR sub-graphs in the training data, in this case \reffig{sailor}. At test time we can do a lookup in this dictionary for any element that gets labeled with a \textbf{DICT} action. Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. This is the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Action & \bf \# Tokens & \bf \% Total \\ \hline
NONE & 41538 & 0.371\\
DICT & 30027 & 0.268 \\
IDENTITY & 19034 & 0.170 \\
VERB & 11739 & 0.104 \\
LEMMA & 5029 & 0.045 \\
NAME & 4537 & 0.04 \\
VALUE & 16  & 0.001\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Distribution of action types in the proxy section of the LDC2014T12 dataset, generated from automatically aligned data. }
\end{table}

Note that \textbf{DICT} counts for around 27\% of the training data, meaning that more than 72\% of tokens can be generated correctly by our action type classifier even if we've never seen them before, which is a huge win.

We believe that \textbf{DICT} should count for much less than 27\%, and \textbf{LEMMA} should count for much more than 4\%, but issues with existing lemmatizers prevent this, see our error analysis in \refsec{errors}.

\Subsection{informativeness}{Action Informativeness Hierarchy}

We define the concept of ``action informativeness'' of an action $a$ as the probability of deriving the correct node from a span of tokens, given that those tokens are labeled with the action $a$, and $a$ is the correct action for that span of tokens.

To provide a concrete example, our dictionary lookup classifier has a test-set accuracy of 0.67. That means that the ``action informativeness'' of the \textbf{DICT} action is 0.67, because given that we correctly label a token as \textbf{DICT}, there is a probability of 0.67 that we correctly generate the corresponding node.

In contrast to \textbf{DICT}, correctly labeling a node as \textbf{IDENTITY}, \textbf{NAME}, and \textbf{NONE} have action informativeness of 1.0, since there is no ambiguity in the node generation once one of those actions have been selected, and we are guaranteed (probability 1.0) to generate the correct node given the correct action.

\Fig{informativeness.png}{0.25}{hierarchy}{Informativeness hierarchy for action tags within AMR.}

This allows us to induce an action informativeness hierarchy, with more informative actions taking precedence over less informative actions for several important tasks. We demonstrate this hierarchy in \reffig{hierarchy}.

\Subsection{data}{Inducing Derivation actions from Training Data}

Given a set of AMR training data, in the form of (graph,sentence) pairs, we first induce alignments from the graph nodes to the sentence, see \refsec{alignment}. Given an alignment, which is an annotation on the graph noting for each node $N_i$ a token $S_j$ that is most likely to have ``generated'' $N_i$, we can induce alignments. For concreteness, imagine the token $S_j$ is ``running'', and the node $N_i$ has the title ``run-01''. For each action type, we can ask whether that action type is able to take token $S_j$ and correctly generate $N_i$. The two action types we find that are able to correctly generate this node are \textbf{DICT} and \textbf{VERB}. We choose the most informative action type of those available to generate the observed node. In this case, that means we choose \textbf{VERB}.

In general, our algorithm is as follows. For all $S_j$ to which no $N_i$ exists such that $N_i$ aligns to $S_j$, assign the action \textbf{NONE} to $S_j$. For all pairs $N_i$, $S_j$, assign $S_j$ the most informative action possible that could have generated $N_i$. \todo{Flesh out discussion of adjacent DICT nodes}

\Subsection{classifier}{Action Classifier}

We use an extremely simple max-ent classifier to make action decisions. The classifier takes as input a pair $< i, S >$, where $i$ is the index of the token in the input sentence, and $S$ is a sequence tokens representing the source sentence. The output of the classifier is a action $T$ such that the likelihood with respect to the data of token $i$ in sentence $S$ generating a node according to the action specified by $T$ is maximized. See Appendix A for a list of classifier features.

\subsection{Test Time Behavior}

At test time, given a sequence of input tokens, we do a simple classification of each token separately, to get a sequence labeling of our input tokens. Then for each token, we apply the behavior associated with the token label, and the resulting set of sub-graphs is passed on to SRL++ for linking.

% \input alignments.tex

\section{Results}

\subsection{End to end results}

Our end to end results are reported by plugging the output of our NER++ into the SRL++ component of JAMR \cite{2014flanigan-amr}, which is able to produce final AMR graphs when given a sequence of spans and their corresponding chunks. AMR parsing accuracy is measured with a metric called smatch \needcite, which stands for ``s(emantic) match''. The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph. We report much higher recall, and a slightly improved F1 score.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|r|}
\hline NER++ & Dataset & P & R & \bf F1 \\ \hline
JAMR & LDC2014T12 & \textbf{0.671} & 0.532 & 0.59 \\
\textbf{Robust} & LDC2014T12 & 0.639 & \textbf{0.596} & \textbf{0.62} \\
JAMR & LDC2014E117 & \textbf{0.669} & 0.529 & 0.59 \\
\bf Robust & LDC2014E117 & 0.636 & \textbf{0.601} & \textbf{0.62} \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Results on two AMR datasets. }
\end{table}

\todo{interpretation}

\subsection{Component results}

On our action-type sequence labeling data generated from automatic alignments on train and test splits of \textit{LDC2014T12}, our classifier achieved a test accuracy of \textbf{0.841}.

The \textbf{DICT} action lookup table achieved an accuracy of \textbf{0.67} on the test set. This is remarkable, given that our model moves many of the difficult semantic tasks onto the \textbf{DICT} tag, and we are using no learning here beyond a simple count of observed span to sub-graph mappings.

% Our approach really shines in domain transfer. Using a system trained on \textbf{LDC2013E117}, which is composed of international newswire, and testing on the web forum subsection \textbf{LDC2014T12}, our system generalizes well.

\Section{errors}{Error Analysis}

\subsection{Weak lemmatization}

The \textbf{DICT} class was intended to be used for things that a system cannot know without memorization, like the nominalization of ``sailor'', see \reffig{sailor}. These don't occur nearly 25\% of the time in the training data. One of the reasons that the \textbf{DICT} class is so disappointingly large is that it's stealing from \textbf{LEMMA}, because AMR will aggressively normalize words and change their part of speech to a semantic neighbor. For example, `gleefully' gets mapped to `glee' and not `gleeful', which is hard to do automatically with stemming rules in the general case. We leave this as a direction for future work.

\subsection{DICT Classifier}

The \textbf{DICT} class is surprisingly large, and our attempts to handle node generation within the class were fairly feeble.

\Section{related}{Related Work}

Beyond the connection of our work with \newcite{2014flanigan-amr}, we note that 
the NER++ component of AMR encapsulates a number of lexical NLP tasks.
These include named entity recognition \cite{2007nadeau-ner,stanford-ner}.

In turn, the SRL++ facet of AMR takes many insights from semantic role labeling
  \cite{2002gildea-srl,2004punyakanok-srl,srikumar2013-srl}, in addition to
  syntactic dependency representations 
  \cite{stanford-dependencies,2005mcdonald-dependency0,2006buchholz-conll}.

More generally, parsing to a semantic representation is has been explored in
  depth for when the representation is a logical form
  \cite{2005kate-semantics,2005zettlemoyer-semantics,2011liang-semantics}.
Recent work by \newcite{2014berant-bio} has used semantic parsing for parsing 
  text into a formal representation of a biological process.

Interlingua
  \cite{1991mitamura-interlingua,1999carbonell-interlingua,1998levin-interlingua}
  are an important inspiration for decoupling the semantics of the AMR language
  from the surface form of the text being parsed; although, AMR has a self-admitted
  English bias.

\section{Future Work}
\subsection{Semantically equivalent POS normalization}
The benefit of this approach could be increased by having a very strong stemmer tuned to AMR parsing, which currently doesn't exist.
\subsection{Etymological approach to node generation}
There is an opportunity to create and test etymologico-semantic approaches to parsing words like `sailor' that would benefit AMR parsing domain generalization tremendously. We envision a system that upon discovering the unseen noun `arbitrageur' at training time is able to retrieve the know verb lemma `arbitrage', and derive that the noun `arbitrageur' probably refers to an entity that is engaged in arbitrage. AMR training data provides an opportunity to perform and measure such a task in isolation, 

\section{Appendix}

\begin{table}[ht]
\small
\centering
\begin{tabular}{|l|rl|}
\hline \bf NER++ Features \\ \hline
Input token\\
Input token word embedding\\
Left token\\
Right token\\
Left bigram\\
Right bigram\\
POS\\
Left POS\\
Right POS\\
Left POS bigram\\
Right POS bigram\\
Token's dependency parent token\\
Token's dependency parent POS\\
Token's dependency parent arc name\\
Bag of outgoing dependency arcs\\
Number of outgoing dependency arcs\\
Number of outgoing dependency arcs (indicator)\\
Max JaroWinker to any lemma in PropBank\\
Closest (JaroWinkler) in PropBank\\
Token NER\\
Left NER bigram\\
Right NER bigram\\
Right NER bigram\\
Indicator for if token is a recognized AMR NER type\\
Indicator for if token is capitalized\\
Parent arc is prep\_* or appos, and parent has NER action\\
Indicator for token is pronoun\\
Indicator for token is part of a coref chain\\
Indicator for token pronoun and part of a coref chain\\
\hline
\end{tabular}
\caption{\label{font-table} The features for the NER++ maxent classifiers. }
\end{table}

% \section*{Acknowledgments}

% The acknowledgments should go immediately before the references.  Do
% not number the acknowledgments section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}

%\begin{thebibliography}{}
%
%\bibitem[\protect\citename{Flanigan \bgroup et al.\egroup}2014]{2014flanigan-amr}
%Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, Noah~A. Smith
%\newblock 2014.
%\newblock {\em ACL 14}, volume~1.
%
%\bibitem[\protect\citename{Banarescu \bgroup et al.\egroup}2013]{Banarescu:13}
%Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider
%\newblock 2013.
%\newblock {\em Proc. of the Linguistic Annotation Workshop and Iteroperability with Discourse}, volume~1.
%
%\bibitem[\protect\citename{Parsons}1990]{Parsons:1990}
%Terence Parsons
%\newblock 1990
%\newblock {\em Events in the Semantics of English: A study in subatomic semantics}.
%\newblock MIT Press.
%
%\bibitem[\protect\citename{Davidson}1967]{Davidson:1967}
%Donald Davidson
%\newblock 1967
%\newblock {\em The Logic of Decision and Action}, pages 81-120.
%\newblock Univ. of Pittsburg Press.
%
%\bibitem[\protect\citename{Pourdamghani}2014]{Pourdamghani:2014}
%Nima Pourdamghani, Yang Gao, Ulf Hermjakob and Kevin Knight
%\newblock 2014
%\newblock {\em Aligning English Strings with Abstract Meaning Representation Graphs}
%\newblock Proc. EMNLP.
%
%\bibitem[\protect\citename{Palmer}2005]{Palmer:2005}
%Palmer M, Kingsbury P, Gildea D
%\newblock 2005
%\newblock {\em The Proposition Bank: An Annotated Corpus of Semantic Roles}
%\newblock Computational Linguistics 31
%
%\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
%Alfred~V. Aho and Jeffrey~D. Ullman.
%\newblock 1972.
%\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
%\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
%
%\bibitem[\protect\citename{Srikumar}2013]{Srikumar:13}
%Vivek Srikumar
%\newblock 2013
%\newblock {\em The Semantics of Role Labeling}
%\newblock ProQuest
%
%\end{thebibliography}

\end{document}

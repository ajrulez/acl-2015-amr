%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathrsfs}
\usepackage{times,latexsym,amsfonts,amssymb,amsmath,graphicx,url,bbm,rotating,datetime}
\usepackage{enumitem,multirow,hhline,stmaryrd,bussproofs,mathtools,siunitx,arydshln}

\input std-macros.tex

\newcommand\w[1]{\textit{#1}} % A textual phrase
\newcommand\e[1]{\textit{#1}} % An AMR edge
\newcommand\n[1]{\textit{#1}} % An AMR node

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Robust Subgraph Generation Improves Abstract Meaning Representation Parsing}
\author{
%  Keenon Werling \\
%  Stanford University\\
%  {\tt keenon@stanford.edu} \\\And
%  Gabor Angeli \\
%  Stanford University\\
%  {\tt angeli@cs.stanford.edu} \\\And
%  Chris Manning \\
%  Stanford University\\
%  {\tt manning@cs.stanford.edu}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}

% The existing AMR corpus is heavily biased to the international newswire domain, making domain transfer of AMR parsing an important challenge. 
% The Abstract Meaning Representation (AMR) is a semantic formalism for which a large and growing set of annotated examples is available.

Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like semantic parsing
  and machine translation.
We identify that node generation, typically done using a simple
  dictionary lookup, is currently the crucial limiting factor in AMR parsing.
% The state-of-the-art AMR parser cannot generate nodes for tokens unseen at training time, limiting its generalization.
We propose a small set of actions to construct parts of the AMR parse from spans of tokens, which allows for more robust learning of this stage.
%the first AMR parser that can handle words at test time that were unseen during training.
We show that our set of construction actions generalize better than the previous approach, even when learned with an extremely simple classifier.
% These actions also provide an insight for an alignment system that yields a ``maximally informative'' set of action labels, which we show yields good results.
We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end F1 from 0.59 to 0.62 on the LDC2013E117 and LDC2014T12 datasets.

\end{abstract}

\section{Introduction}

\Fig{glee.png}{0.20}{glee}{The AMR graph for 
\w{He gleefully ran to his dog Rover}.
We show that improving the generation of low level subgraphs
  (e.g., \w{Rover} generating \n{name} $\xrightarrow{:op1}$ \n{``Rover''}) significantly improves
  end-to-end performance.
}
Abstract Meaning Representation (AMR) \cite{2013banarescu-amr} is a rich graph-based language for expressing semantics over a broad domain.
% AMR is written as a directed, rooted graph merging propositional logic and neo-Davidsonian semantics (\todo{cite}).
\reffig{glee} shows an example AMR for ``he gleefully ran to his dog Rover'', and we give a brief tutorial on AMR in \refsec{crash}.

AMR parsing is the task of mapping a natural language sentence into an AMR graph.
AMR parsing is exciting because a practical broad-domain AMR parser could enable a new breed of natural language applications ranging from semantically aware MT to rich broad-domain QA over text-based knowledge bases.
At the time of this writing AMR is the target of a multi-institution multi-year data-labeling program led by Kevin Knight, which promises to produce a corpus that is large enough to make it possible to parse to AMR well despite its many challenges.

\FigStar{method.png}{0.23}{method}{Derivation process for ``He gleefully ran to his dog Rover''. First the tokens in the sentence are labeled with derivation actions, then those actions are used to generate AMR sub-graphs, and then those sub-graphs are stitched together to form a coherent whole.}

However, no matter how much data we are soon to have, AMR parsing remains a hard unsolved task, with only modest performance numbers reported so far.
A good AMR parser needs to be able to handle a broad range of semantic interpretation tasks, and 
AMR also possesses distinctive computational challenges from structural properties such as providing no guarantees about projectivity or even acyclicity of its graphs.

We follow previous work \cite{2014flanigan-amr} in dividing AMR parsing into two steps. 
The first step is \textit{concept identification}, which generates concept (or entity) nodes from text, and which we'll refer to as \textit{NER++} (\refsec{ner}). 
The second step is \textit{relation identification}, which adds arcs to link these nodes into a fully connected AMR graph, which we'll call \textit{SRL++} (\refsec{srl}).

Our initial assumption (which we suspect would be shared by many researchers with a background in structured prediction) was to assume that the main area deserving further attention in AMR parsing is predicting the edges in the not well-aligned, cyclic non-projective graphs of AMR (i.e., SRL++).
This intuition follows by analogy from the locus of work for syntactic dependency parsing. We spent months testing novel structure-prediction algorithms and training regimes to capture aspects of the non-projective cyclic graphs, only to find that the careful application of MST presented in \newcite{2014flanigan-amr} set an extremely strong baseline for the SRL++ task, and our gains were negligible to non-existent.

Over time we realized that SRL++ is \textit{not} the hard part of AMR parsing. The hard part of AMR parsing is NER++. A first piece of evidence for this claim comes from a close analysis of the only published AMR parser to date, \cite{2014flanigan-amr}, dubbed ``JAMR''.
When JAMR parses using its own NER++ and SRL++ components, it gets an end-to-end score of 0.58 F1.
However, when JAMR is given a gold NER++ output, and must only perform SRL++ over given sub-graphs it scores 0.80 F1 against a virtual upper bound of 0.83 F1, which is inter-annotator agreement on AMR \todo{NEED CITE FOR THIS}.  (And annotators very rarely disagree on NER++, which amounts to disagreeing about the entities mentioned in a sentence). \todo{verify -- CDM: I'm not sure this is true} [since surely there is quite a bit of inconsistency in NER++, such as for which compound words with derivational morphology to expand into subgraphs.]
So this means the SRL++ component is nearly perfect, if given perfect NER++.

In retrospect, it makes sense that SRL++ within AMR is \textit{relatively} easy given a perfect set of nodes to link together.
There's a strong type-check feature for the existence and type of any arc just by looking at its end-points, and syntactic dependency features are very informative for removing any remaining ambiguity.
If a system is considering how to link the node ``run-01'' in \reffig{glee}, the verb-sense frame for ``run-01'' leaves very little entropy in terms of what we could assign as an ARG0 arc.
It must be a noun, which leaves either ``he'' or ``dog'', and this is easily decided in favor of ``he'' by looking for an nsubj arc in the dependency parse. Given a modest amount of data, a simple parser can learn these lexical and syntactic type-check rules.

While SRL++ is hard, NER++ is extremely challenging, encompassing many SemEval tasks in a single monolith.
This task was handled by a simple baseline system in \cite{2014flanigan-amr}, which memorized a purely lexical mapping from spans of text to AMR nodes, augmented by an NER system and time expression regex at test time.
This is problematic, because fully 38\% of the tokens in the test set are unobserved at training time, and the existing method has no way to handle generation for them.

The primary contribution of this paper is a method to largely delexicalize NER++ to gracefully handle unseen tokens.
Choosing from among a small set of `generative actions' our system can derive an AMR sub-graph from a span of tokens (see \reffig{method}).
For example, we have an action \textit{VERB} that will perform a verb-sense-disambiguation to the appropriate PropBank frame \cite on the source token, like the ``ran'' to ``run-01'' example in \reffig{glee}.

We show that this approach improves recall dramatically over previous approaches, and that end to end performance is improved from 0.59 to 0.62 smatch when our generative actions are stitched together by the previous state of the art parser \cite{2014flanigan-amr}.
We suspect that the gains are not even bigger mainly because our NER++ system is able to generate nodes that the SRL++ system has never seen during training time, which makes the still lexicalized typecheck features of the SRL++ component useless.

% Previous work has observed that AMR parsing can be partitioned into two tasks: a rich lexically grounded entity detection system, which we call NER++ (see \refsec{ner}), and a relationship detection system, which we call SRL++ (see \refsec{srl}).

% As an example of the distinction, let's briefly go over \reffig{glee}, the AMR for ``he gleefully ran to his dog Rover''. To produce this parse, the NER++ task will have to do a verb sense disambiguation on ``ran'' to get ``run-01'', and a lemmatization on ``gleefully'' to ``glee''. Then NER++ will have to recognize ``Rover'' is a name, and generate the sub-graph (name :op1 ``Rover''). It will also have to recognize that ``to'' is a dropped preposition, and so doesn't get a node, and ``his'' is a coreferant that also doesn't get a node. Then ``dog'' and ``he'' will have to be generated directly as nodes. The SRL++ task is then responsible for linking together the output of the NER++ task to produce a fully-connected semantic graph.

% At test time, AMR parsing is commonly divided into two tasks, which we refer to as NER++ and SRL++.

% Valid examples to use:
% excessively -> excessive
% mother -> mother
% exhaustive -> exhaust-01

% For instance, the adjective \textit{exhaustive} in ``the \textit{exhaustive} search'' should be parsed as an AMR verb, \textit{exhaust-01}, but JAMR is unable to recognize this because it has no instance of the adjective \textit{exhaustive} used in the training data. Our system can generalize from training data to learn that \textit{exhaustive} is to be treated as a verb, and match against the OntoNotes \needcite database of verb sense frames at test time to produce \textit{exhaust-01}.

% In order to train our system, we need a correspondence between AMR nodes and their source tokens in the training data. For example, a node with the title ``glee'' will almost certainly represent the token ``gleefully'' if that appears in the source text. However, AMR training data is ``unaligned'', meaning that no effort is made to annotate which token in a sentence is being represented by a given node in an AMR graph.

% Automatically inferring alignments is a source of noise in training our system.
% We also propose a novel alignment system inspired by our generative actions to explicitly minimize that noise, using a metric we define as ``action informativeness''.
% We cast AMR alignment as the task of finding the alignment of AMR graph to the source sentence that maximizes the informativeness of the implied generative actions.
% We define this further in \refsec{alignment}.

\Section{crash}{A Crash-Course in AMR}

AMR is a language for expressing semantics as a rooted, directed, and potentially cyclic graph, where nodes represent concepts and arcs are relationships between concepts.
The nodes (concepts) in an AMR graph do not have to be explicitly grounded in the source sentence, and while such an alignment can often be generated it is not provided in the training corpora.
The semantics of nodes can represent lexical items (e.g., \w{dog}), sense tagged lexical items (e.g., \textit{run-01}), type markers (e.g., \textit{date-entity}), and a host of other phenomena.

The edges (relationships) in AMR describe one of a number of semantic relationships between concepts.
The most salient of these is semantic role labels, such as the ARG0 and destination arcs in \reffig{method}.
However, often these arcs define a semantics more akin to syntactic dependencies (e.g., \textit{mod} standing in for adjective and adverbial modification), or take on domain-specific semantics (e.g., the month, day, and year arcs of a \textit{date-entity}).

AMR is based on neo-Davidsonian semantics, \cite{Davidson:1967,Parsons:1990}.
To introduce AMR and its notation in more detail, we'll unpack the translation of the sentence ``he gleefully ran to his dog Rover''. 
We show in \reffig{glee} the interpretation of this sentence as an AMR graph.

%AMR makes no effort to have a one-to-one correspondence between nodes in a graph and tokens in the sentence whose semantics is being represented.
%In fact, AMR will often expand single tokens into large sub-graph elements, or ignore tokens completely.
%It's important to understand that AMR represents the relationships between objects referred to by the surface text, \textit{not} the relationships between the words themselves.

% JON: Kind of inconvenient that Figure 2 is not on the same page as the explanation..
The root node of the graph is labeled \n{run-01}, corresponding to the PropBank \cite{Palmer:2005} definition of the verb \w{ran} .
%This is the name of a verb sense definition drawn from PropBank \needcite for the sense of the verb ``ran'' in this sentence.
\w{run-01} has an outgoing \e{ARG0} arc to a node \w{he}, with the usual PropBank semantics.
%semantics (drawn from the PropBank frame) that roughly correspond to ``he'' being the doer of the ``run-01'' action. 
The outgoing \e{mod} edge from \n{run-01} to \n{glee} takes a general purpose semantics corresponding to adjective, adverbial, or other modification of the governor by the dependent.
We note that \n{run-01} has a \e{destination} arc to \n{dog}.
The label for \e{destination} is taken from a finite set of special arc sense tags similar to the preposition senses found in \cite{Srikumar:2013}.
The last portion of the figure parses \w{dog} to a node which will eventually serve as a type marker, and \w{Rover} into the larger subgraph indicating a concept with name ``Rover.''
%Then we have a section of the graph that is best interpreted as a unit, where all of the children of ``dog'' effectively mean that ``dog'' has the name ``Rover.''


\Subsection{formal}{Formal task definition}
Formally, an AMR graph is represented as a directed multigraph.
Following \newcite{2014flanigan-amr}, we simplify the representation by (1) disallowing
  cycles in the graph, and (2) disallowing multiple edges between two nodes in a
  graph.
Both of these are rare phenomena in the representation, and substantially complicate
  the task.

Thus, we treat AMR as a directed graph $G$ of nodes $\bN = \{n_0 \dots n_k\}$, and
  an edge matrix $\bA$ such that $\bA_{i,j} = l$ means that an arc exists from $n_i$ to $n_j$ with label $l$;
  $\bA_{i,j} = \e{NONE}$ entails that no arc exists between the nodes.
  $\bA$ is over a label space $\bL$: $\bA \in \bL^{k \times k}$.
Each label $l \in \bL$ is one of the valid arc labels between two nodes (e.g.,
  \e{ARG0}, \e{mod}), in addition to a special label denoting the lack of an arc, \e{NONE}.

%Formally, given an array of tokens $S = [s_0, \ldots, s_n]$, we generate a directed AMR graph $G$, defined as the pair $(N = [n_0, \ldots, n_k], A \in L^{k*k})$, where $N$ is an array of AMR nodes (which doesn't have to be the same length or have a clear correspondence to $S$), and $A$ is a matrix of labels $L$, where $A_{i,j} = l \in L$ means that an arc exists from node $n_i$ to node $n_j$ with label $l$ in the parsed graph. We include the special label ``NONE'' in $L$, corresponding to no arc existing between two nodes.

\Subsection{sub-chunks}{AMR Subgraphs}
% One-to-many mapping
The mapping from tokens of a sentence to AMR nodes is not one-to-one.
A single token or span of tokens can generate a \textit{subgraph} of AMR consisting
  of multiple nodes.
%AMR contains components that, while they may be composed of multiple nodes, 
These subgraphs can logically be considered the expression of a single concept,
  and are useful to treat as such (e.g., see \refsec{ner}).
%For the NER++ task, we would like to be able to generate these single concept subgraphs directly from spans of text.

% Figure: Sailors 
\Fig{sailor.png}{0.25}{sailor}{AMR representation of the word \w{sailor}, which is notable for breaking the word up into a self-contained multi-node unit unpacking the derivational morphology of the word.}

% Figure: Time expression 
\Fig{date.png}{0.25}{date}{AMR representation of the span \w{January 1, 2008}, an example of how AMR can represent structured data by creating additional nodes like \n{date-entity} to signify the presence of special structure}

% Structured data
Many of these multi-node subgraphs capture structured data such as time expressions
  (\reffig{date}).
%AMR can also capture structured data, like time expressions, see \reffig{date}. 
In this example, a \n{date-entity} node is created to signify that this cluster of 
  nodes is part of a structured sub-component representing a date, where the nodes
  and arcs within the component have specific semantics.
This illustrates a broader recurring pattern in AMR, which is to have an 
  artificial node with certain expected children with special semantics.
A particularly salient example of this pattern is the \n{name} node (see ``Rover'' in \reffig{glee}) which signifies 
  that all outgoing arcs with label \e{op} comprise the tokens of a name object.

% Hard cases
The ability to decouple the meaning representation of a lexical item from its
  surface form allows for rich semantic interpretations of certain concepts
  in a sentence.
%AMR makes an attempt to capture some semantic meanings in words that are 
%  difficult to capture in a way that is not domain specific. 
% JON: reword "difficult to capture in a more difficult to capture in a more principled way than dictionary lookups"
% JON: plus, what do "dictionary lookups" look like?
For example, the token \w{sailor} is represented in \reffig{sailor} by a concept graph 
  representing a person who performs the action \n{sail-01}. 
Whereas often the AMR node aligned to a span of text is a straightforward function
  of the text, these cases remain difficult to capture in a more principled way than
  dictionary lookups.
%This is difficult to model without resorting to memorization, because the 
%  etymological clues are so sparse. 
%We note this as an area for further exploration.


\section{Task decomposition}


% Two stages

To the best of our knowledge, the JAMR parser \cite{2014flanigan-amr} is
the only published end-to-end AMR parser.
The crucial insight in JAMR is that AMR parsing can be broken into two 
relatively distinct tasks: (1) \textbf{NER++} -- the task of interpreting what entities are being referred to in 
the text, realized by generating the best AMR sub-graphs for a given set of tokens, and
(2) \textbf{SRL++} -- the task of discovering what 
relationships those entities have between one another other, realized by taking the disjoint subgraphs generated
  by NER++ and creating a fully-connected graph.
We describe both tasks in more detail below.
%We define the terms \textbf{NER++} and \textbf{SRL++} in more detail within this section.

% +++
% We adopt the dual decomposition based MST algorithm from the parser, 
%   and contribute primarily towards replacing
%  the NER++ component.
%---

\Subsection{ner}{NER++}
% JON: s/To illustrate/For example/ ?
Much of the difficulty of parsing to AMR lies in generating local sub-graphs representing the meaning of token spans.
For instance, the formalism implicitly demands rich notions of NER, lemmatization, word sense disambiguation, number normalization, and temporal parsing; among others.
To illustrate, \reffig{method} requires lemmatization (\textit{gleefully} $\rightarrow$ \textit{glee}), word sense tagging (\textit{run} $\rightarrow$ \textit{run-01}), and open domain NER (i.e., \textit{Rover}); \reffig{date} shows an example of temporal parsing.
Furthermore, many of the generated subgraphs (e.g., \textit{sailor} in \reffig{sailor}) have rich semantics beyond those produced by standard NLP systems.

% JON: This is not formal compared to the formal-ness of previous sections (e.g. definition of A)
% We refer to this task as NER++ (see \reffig{method}).
Formally, this is the task of generating a disjoint set of subgraphs representing the meanings of localized spans of words in the sentence.
% NOTE: not quite true, SRL++ uses dependency path features that rely on alignments
% This is also the task where AMR alignment is useful
% Whereas the structure of the AMR graphs themselves provide data for training the SRL++ model, the NER++ model requires an alignment between nodes of the AMR graph and the surface form of the sentence.
For NER++, JAMR uses a simple Viterbi sequence model to directly generate AMR-subgraphs from memorized mappings of text spans to subgraphs. The main contribution of this paper is our exploration of a better way of doing NER++ in \refsec{nerplusplus}.

%Parsing to the AMR representation demands a rich NER system, word sense disambiguation, number normalization, time parsing, and many semantic nominalizations and part of speech translations, and within-sentence coreference. We refer to these ``low-level'' AMR tasks collectively as NER++. NER++ is the sub-task of generating the best AMR sub-graphs (``sub-graph'' is defined in \refsec{sub-chunks}) given the set of tokens $S$. This involves both partitioning the source text into spans that will be rendered as a single sub-graph in AMR (e.g. ``run'', ``People's Republic of China'', ``January 1, 2008''), and then mapping each of those spans into a corresponding AMR sub-graph of maximum likelihood.

\Subsection{srl}{SRL++}
The second stage of the AMR decomposition consists of generating a coherent graph
  from the set of disjoint sub-graphs produced by NER++.
Unlike the domain-specific arcs within a single sub-graph produced by NER++, the
  arcs in SRL++ tend to have generally applicable semantics.
For example, the SRL arcs (e.g., \e{ARG0} and \e{destination} in \reffig{method}),
  or the syntactic dependency arcs (e.g., \e{mod} and \e{poss} in \reffig{method}).
For SRL++, JAMR uses a variation of the maximum spanning tree algorithm augmented by dual decomposition to impose linguistically motivated constraints on a maximum likelihood stitching. JAMR's SRL++ component is extremely effective, and we were unable to produce a better SRL++ system in our experiments with several other structured prediction approaches.
Therefore, we use the modified MST algorithm implemented in \newcite{2014flanigan-amr} to produce
  a labeled DAG corresponding to the relation edges.

%Given a perfect NER++ system for an AMR parser, there remains the task of noting the verb arguments, preposition sense actionging, and doing some augmented semantic dependency parsing in order to join the disjoint NER++ output into a single AMR parse. We call this task SRL++. SRL++ is the sub-task of taking as input the disjoint sub-graphs generated by NER++, and adding the maximum likelihood set of arcs between the sub-graphs in order to have a fully connected graph.


\section{A Novel NER++ Method}\label{sec:nerplusplus}
Our approach to improving NER++ comes from a simple idea: instead of trying to pick 
  which of thousands of AMR sub-graphs to generate from a span of text directly, 
  we partition the AMR sub-graph search space in terms of the actions needed to 
  derive a node from its aligned token. 
At test time we do a sequence labeling of input tokens with these actions, and 
  then deterministically derive the AMR sub-graphs from spans of tokens by applying 
  the transformation decreed by their actions. 
This dramatically reduces sparsity, and improves generalization (and consequently
  recall) on the test set, without noticable affecting precision.
To illustrate, 38\% of the words in the LDC2014E113 test set are 
  unseen during training time.
We explain in \refsec{actions} how exactly we manage this partition, and explain in \refsec{data} how we create training data from existing resources to train an action-type classifier. Then we setup the classifier itself in \refsec{classifier}.

\Subsection{actions}{Derivation actions}

We partition the AMR sub-graph space into a set of 7 actions, each corresponding to an action that will be taken by the NER++ system if a token receives this classification.

\paragraph{VERB} This action capture the verb-sense disambiguation feature of AMR. To execute on a token, we look for the most similar PropBank frame, make that the title of the corresponding node.

\paragraph{IDENTITY} This action handles the common of tokens) case that the title of the node corresponding to a token is identical to the source token. To execute, we take the lowercased version of the token to be the title of the corresponding node.

\paragraph{VALUE} This action interprets a token by its integer value. 
The AMR representation is sensitive to the difference between a node with a title
  of \n{5} (the integer value) and ``5'' or ``five'' -- the string value.
This is a rare action, but is nontheless distinct from any of the other classes.
%In AMR a node with title ``five'' and a node with title ``5'' are two distinct entities, with different semantic interpretation. 
We execute this action by getting an integer value with a regex based number normalizer, and using that as the title of the generated node.

\paragraph{LEMMA} AMR often performs stemming and POS transformations on the source token in generating a node. 
For example, we get \n{glee} from \w{gleefully}.
We try to capture this by a \textbf{LEMMA} action, which is executed by using the lemma of the source token as the generated node title.
Note that this does not capture all lemmatizations, as there is often a discrepancy
  between the lemma generated by the lemmatizer and the correct AMR lemma.

\paragraph{NONE} This action corresponds to ignoring this token, in the case that
  the node should not align to any corresponding AMR fragment.
%37\% of the time, we can just ignore the token.

\paragraph{NAME} A common structured data type in AMR is the \n{name} construction. 
For example, \w{Rover} in \reffig{glee}.
We can capture this phenomenon on unseen names by attaching a created \n{name} node to the top of this span.

\paragraph{DICT} This class serves as a backoff for the other classes, implementing
an approach similar to \newcite{2014flanigan-amr}.
%This is basically our catch-all.
In particular, we memorize a simple mapping from spans of text
  (like \w{sailor}) to their corresponding most frequently seen AMR sub-graphs 
  in the training data (i.e., the graph in \reffig{sailor}).
At test time we can do a lookup in this dictionary for any element that gets 
  labeled with a \textbf{DICT} action. 
If an entry is not found in the mapping, we back off to the second most common
  class produced by the classifier.
%Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. 
%\reftab{distro} gives the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).


%
% ACTION INFORMATIVENESS
%
\Subsection{informativeness}{Action Reliability}

% Introduce informativeness
Although some of our actions are entirely deterministic in their conversion
  from the word to the AMR sub-graph (e.g., IDENTITY), others are prone to
  making mistakes in this conversion (e.g., VERB, DICT).
We define the notion of \textit{action reliability} of an action as the 
  probability of deriving the correct node from a span of tokens, 
  given that those tokens are labeled with the correct action.

To provide a concrete example, our dictionary lookup classifier predicts the correct
  AMR sub-graph 67\% of the time on the test set.
We therefore define the reliability of the DICT action is 0.67>
%, because given that we correctly label a token as \textbf{DICT}, there is a probability of 0.67 that we correctly generate the corresponding node.
In contrast to DICT, correctly labeling a node as IDENTITY, NAME, and NONE have 
  action reliability of 1.0, since there is no ambiguity in the node generation 
  once one of those actions have been selected, 
  and we are guaranteed to generate the correct node given the correct action.

\Fig{informativeness.png}{0.25}{hierarchy}{
Reliability of each action.
The top row are actions which are deterministic;
  the second row are actions which occasionally produce errors.
DICT is the least prefered action, with a relatively high error rate.
\todo{replace I with $p$}
}

We can therefore construct a hierarchy of reliability -- all else being equal, we
  prefer to generate actions from higher in the hierarchy, as they are more likely
  to produce the correct sub-graph.
We show this hierarchy in \reffig{hierarchy}.
This hierarchy is used in two places: (1) during training, when two actions could
  generate the aligned AMR node we prefer the more reliable one; and 
  (2) in our aligner, we bias alignments towards generating more reliable action
  sequences (see \refsec{align}).

%This allows us to induce an action informativeness hierarchy, with more informative actions taking precedence over less informative actions for several important tasks. We demonstrate this hierarchy in \reffig{hierarchy}.


%
% ACTION DISTRIBUTION
%
\Subsection{actions}{Distribution of Actions}

% Distribution of Actions
\begin{table}[h]
\begin{center}
\begin{tabular}{l|rr}
\bf Action & \bf \# Tokens & \bf \% Total \\ \hline
NONE & 41538 & 37.1\\
DICT & 30027 & 26.8 \\
IDENTITY & 19034 & 17.0 \\
VERB & 11739 & 10.4 \\
LEMMA & 5029 & 4.5 \\
NAME & 4537 & 4.0 \\
VALUE & 16  & 0.1\\
\end{tabular}
\end{center}
\caption{\label{tab:distro} Distribution of action types in the proxy section of the LDC2014T12 dataset, generated from automatically aligned data. }
\end{table}

% Intro
We analyze the empirical distribution of actions in our automatically 
  aligned corpus in \reftab{distro}.
The most common class of action is the NONE action.
This is not unexpected, as many of the tokens in the source sentence are either
  considered semantically meaningless by the AMR representation (e.g.,
  \w{a}, \w{the}), or serve as lexical grounding for an AMR arc (e.g.,
  prepositions).

% Dict
The second highest class is the DICT action.
Recall that prior work uses dictionary lookup for every lexical item in the
  sentence, which is equivalent to a distribution of 100\% on this class.
A number of the elements here are truly complex constructions; but, this also
  serves as the implicit action for when other categories fail to generate the
  correct action during training (e.g., the lematizer).
Therefore, it is likely that the frequency of this action could be pushed down
  further by tuning the other actions more carefully.

% Others
The cumulative frequency of the identity, name, and value actions is particularly 
  notable as the AMR sub-graph corresponding to these actions are
  always generated deterministically, and therefore correctly by our algorithm.
Therefore, these are trivial gains in the case where a word is unseen in the
  training corpus.

%We believe that \textbf{DICT} should count for much less than 27\%, and \textbf{LEMMA} should count for much more than 4\%, but issues with existing lemmatizers prevent this, see our error analysis in \refsec{errors}.


%It's not always possible to derive an AMR sub-graph directly from tokens at test 
%  time without having memorized a mapping. 
%For example, the parse of \w{sailor} as ``person who sails'' (\reffig{sailor}),
%  is nearly impossible without some form of memorization. 
%That's where the \textbf{DICT} class is important.

%To implement a \textbf{DICT} class, we memorize a simple mapping from spans of text, like ``sailor'' to their corresponding most frequently seen AMR sub-graphs in the training data, in this case \reffig{sailor}. At test time we can do a lookup in this dictionary for any element that gets labeled with a \textbf{DICT} action. Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. \reftab{distro} gives the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).


%Note that \textbf{DICT} counts for around 27\% of the training data, meaning that more than 72\% of tokens can be generated correctly by our action type classifier even if we've never seen them before, which is a huge win.

%
% TRAINING
%
\Subsection{data}{Training the Action Classifier}

\begin{table}[t]
\small
\begin{center}
\begin{tabular}{l}
Input token; word embedding                  \\
Left+right token / bigram                    \\
POS; Left+right POS / bigram                 \\
Dependency parent token / POS                \\
Incoming dependency arc                      \\
Bag of outgoing dependency arcs              \\
Number of outgoing dependency arcs           \\
Max JaroWinker to any lemma in PropBank      \\
Closest (JaroWinkler) in PropBank \todo{???} \\
NER; Left+right NER / bigram                 \\
Capitalization                               \\
Incoming prep\_* or appos + parent has NER   \\
Token is pronoun                             \\
Token is part of a coref chain               \\
Token pronoun and part of a coref chain     \\
\end{tabular}
\end{center}
\caption{\label{tab:features} The features for the NER++ maxent classifier. }
\end{table}

Given a set of AMR training data, in the form of (graph,sentence) pairs,
  we first induce alignments from the graph nodes to the sentence 
  (see \refsec{alignment}).
Formally, for each node in the AMR graph $n_i$ we find a token $s_k$ in the
  sentence.
%With this alignment, which is an annotation on the graph noting for each node $N_i$ 
%  a token $S_j$ that is most likely to have ``generated'' $N_i$, we can induce 
%  alignments. 
For each action type, we can ask whether that action type is able to take 
  token $s_k$ and correctly generate $n_i$. 
For concreteness, imagine the token $s_k$ is \w{running}, and the node $n_i$ has 
  the title \n{run-01}. 
In our example, the two action types we find that are 
  able to correctly generate this node are DICT and VERB. 
We choose the most reliable action type of those available (see \reffig{hierarchy})
  to generate the observed node -- in this case, VERB.
  
This provides a noisy supervised training set for our action classifier.
We then train a simple maxent classifier to make action decisions at each node. 
At test time,
  the classifier takes as input a pair $\langle i, S \rangle$, where $i$ is the 
  index of the token in the input sentence, and $S$ is a sequence tokens 
  representing the source sentence.
It then uses the features in \reftab{features} to predict the actions to take at
  that node.

%At test time, given a sequence of input tokens, we do a simple classification of each token separately, to get a sequence labeling of our input tokens. Then for each token, we apply the behavior associated with the token label, and the resulting set of sub-graphs is passed on to SRL++ for linking.

%In general, our algorithm is as follows. For all $S_j$ to which no $N_i$ exists 
%  such that $N_i$ aligns to $S_j$, assign the action \textbf{NONE} to $S_j$.
%For all pairs $N_i$, $S_j$, assign $S_j$ the most informative action possible 
%  that could have generated $N_i$. \todo{Flesh out discussion of adjacent DICT nodes}
%
%\Subsection{classifier}{Action Classifier}
%
%The output of the classifier is an action $T$ such that the likelihood with respect to the data of token $i$ in sentence $S$ generating a node according to the action specified by $T$ is maximized. See Appendix A for a list of classifier features.
%
%\todo{How do you work out verb senses?}
%
%\subsection{Test Time Behavior}


\input alignments.tex


\Section{related}{Related Work}

Beyond the connection of our work with \newcite{2014flanigan-amr}, we note that 
the NER++ component of AMR encapsulates a number of lexical NLP tasks.
These include named entity recognition \cite{2007nadeau-ner,stanford-ner},
  word sense disambiguation \cite{1995yarowsky-wsd,2002banerjee-wsd},
  lemmatization, and a number of more domain specific tasks.
For example, a full understanding of AMR requires normalizing temporal
  expressions \cite{2010verhagen-tempeval,2010strotgen-temporal,2012chang-temporal}.
 
In turn, the SRL++ facet of AMR takes many insights from semantic role labeling
  \cite{2002gildea-srl,2004punyakanok-srl,srikumar2013-srl} to capture the
  relations between verbs and their arguments.
In addition, many of the arcs in AMR have nearly syntactic interpretations
  (e.g., \e{mod} for adjective/adverb modification, \e{op} for compound noun
  expressions).
These are similar to representations used in syntactic dependency parsing
  \cite{stanford-dependencies,2005mcdonald-dependency0,2006buchholz-conll}.

More generally, parsing to a semantic representation is has been explored in
  depth for when the representation is a logical form
  \cite{2005kate-semantics,2005zettlemoyer-semantics,2011liang-semantics}.
Recent work has applied semantic parsing techniques to representations beyond
  lambda calculus expressions.
For example, work by \newcite{2014berant-bio} parses
  text into a formal representation of a biological process.
\newcite{2014hosseini-algebra} solves algabreic word problems by parsing them
  into a structured meaning representation.
In contrast to these approaches, AMR attempts to capture open domain semantics
  over arbitrary text.

Interlingua
  \cite{1991mitamura-interlingua,1999carbonell-interlingua,1998levin-interlingua}
  are an important inspiration for decoupling the semantics of the AMR language
  from the surface form of the text being parsed; although, AMR has a self-admitted
  English bias.




\section{Results}

\subsection{End to end results}

Our end-to-end results are reported by plugging the output of our NER++ into the SRL++ component of JAMR \cite{2014flanigan-amr},\footnote
  {Available at \url{https://github.com/jflanigan/jamr}.}
which is able to produce final AMR graphs when given a sequence of spans and their corresponding chunks. AMR parsing accuracy is measured with a metric called smatch \needcite, which stands for ``s(emantic) match''. The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph. We report much higher recall, with negligible ($\leq .01$) loss in precision.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|llr|}
\hline NER++ & Dataset & P & R & F1 \\ \hline
JAMR & 2014T12 & \textbf{0.671} & 0.532 & 0.593 \\
JAMR & 2013E117 & \textbf{0.669} & 0.529 & 0.591 \\
\textbf{Robust} & 2014T12 & 0.666 & \textbf{0.583} & \textbf{0.622} \\
\bf Robust & 2014E117 & 0.659 & \textbf{0.590} & \textbf{0.623} \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Results on two AMR datasets for JAMR \cite{2014flanigan-amr} and Robust (our system).}
\end{table}


\begin{table}[h]
\begin{center}
\begin{tabular}{|l|llr|}
\hline Dataset & P & R & F1 \\ \hline
LDC2014T12 & 0.839 & 0.765 & 0.800 \\
LDC2014E117 & 0.847 & 0.776 & 0.810 \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Results on two AMR datasets when given gold NER++. }
\end{table}

\todo{interpretation}

\subsection{Component results}

On our action-type sequence labeling data generated from automatic alignments on train and test splits of LDC2014T12, our classifier achieved a test accuracy of \textbf{0.841}.

The \textbf{DICT} action lookup table achieved an accuracy of \textbf{0.67} on the test set. This is remarkable, given that our model moves many of the difficult semantic tasks onto the \textbf{DICT} tag, and we are using no learning here beyond a simple count of observed span to sub-graph mappings.

% Our approach really shines in domain transfer. Using a system trained on \textbf{LDC2013E117}, which is composed of international newswire, and testing on the web forum subsection \textbf{LDC2014T12}, our system generalizes well.

\Section{errors}{Error Analysis}

\subsection{Weak lemmatization}

The \textbf{DICT} class was intended to be used for things that a system cannot know without memorization, like the nominalization of ``sailor'', see \reffig{sailor}. These don't occur nearly 25\% of the time in the training data. One of the reasons that the \textbf{DICT} class is so disappointingly large is that it's stealing from \textbf{LEMMA}. This is because AMR will generally normalize a word to its root by removing derivational morphology. For example, `gleefully' gets mapped to `glee', rather than being left alone or just mapped to `gleeful'. While appropriate lexical resources should allow us to generate the root for such derived words, in practice we only had available to us a lemmatizer which handled inflectional morphology. We leave this as a direction for future work.

\subsection{DICT Classifier}

The \textbf{DICT} class is surprisingly large, and our attempts to handle node generation within the class were fairly feeble.

\section{Future Work}
\subsection{Semantically equivalent POS normalization}
The benefit of this approach could be increased by having a very strong stemmer tuned to AMR parsing, which currently doesn't exist.

\subsection{Morphological approach to node generation}
There is an opportunity to create and test derivational morphology components in parsing words like `sailor' that would benefit AMR parsing domain generalization tremendously. We envision a system that upon discovering the unseen noun `arbitrageur' at training time is able to retrieve the known verb root `arbitrage', and to derive that the noun `arbitrageur' probably refers to an entity that is engaged in arbitrage. AMR training data provides an opportunity to perform and measure such a task in isolation, 

% \section{Appendix}

% \section*{Acknowledgments}

% The acknowledgments should go immediately before the references.  Do
% not number the acknowledgments section. Do not include this section
% when submitting your paper for review.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{ref}

\end{document}

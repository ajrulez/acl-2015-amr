%
% File acl2015.tex
%
% Contact: car@ir.hit.edu.cn, gdzhou@suda.edu.cn
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2015}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{bbm}
\usepackage{mathrsfs}

\input std-macros.tex

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Robust Subgraph Generation for Abstract Meaning Representation Parsing}
\author{Keenon Werling \\
  Stanford University\\
  {\tt keenon@stanford.edu} \\\And
  Gabor Angeli \\
  Stanford University\\
  {\tt gabor@stanford.edu} \\\And
  Chris Manning \\
  Stanford University\\
  {\tt manning@stanford.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}

% The existing AMR corpus is heavily biased to the international newswire domain, making domain transfer of AMR parsing an important challenge. 
% The Abstract Meaning Representation (AMR) is a semantic formalism for which a large and growing set of annotated examples is available.

Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics.
Current AMR parsers cannot generate nodes for tokens unseen at training time, limiting their generalization.
We propose a small set of actions to construct sub-graphs from a spans of tokens, which solves this problem.
We show that our set of construction actions generalize better than the previous approach, even when learned with an extremely simple classifier.
These actions also provide an insight for an alignment system that yields a ``maximally informative'' set of action labels, which we show yields good results.
We improve on published state-of-the-art AMR parsing, boosting end-to-end F1 from 0.59 to 0.62 on the LDC2013E117 and LDC2014T12 datasets.

\end{abstract}

\section{Introduction}

\Fig{glee.png}{0.25}{glee}{AMR graph for ``He gleefully ran to his dog Rover''. Nodes represent concepts, and arcs are relationships between concepts. The dark arc labeled ``op1'' is expected to be generated by NER++.}

Abstract Meaning Representation (AMR) \cite{Banarescu:13} is a rich graph-based language for expressing semantics over a broad domain.
\reffig{glee} shows an example AMR for ``he gleefully ran to his dog Rover''.
For more information, we give a brief tutorial on AMR in \refsec{crash}.
When we are able to parse to AMR well across broad domains, its extreme expressiveness promises to power a new breed of natural language applications ranging from semantically aware MT to rich broad-domain QA over text-based knowledge bases.
The first practical AMR parser may not be far off, as at the time of this writing AMR is the target of a multi-institution multi-year data-labeling program led by Kevin Knight, which promises to produce a corpus that is large enough to make it possible to parse to AMR well despite its inherently lexical nature.

\FigStar{method.png}{0.23}{method}{Derivation process for ``He gleefully ran to his dog Rover''. First the tokens in the sentence are labeled with derivation actions, then those actions are used to generate AMR sub-graphs, and then those sub-graphs are stitched together to form a coherent whole.}

No matter how much data we are soon to have, though, AMR parsing remains a hard unsolved task, with only modest performance numbers reported so far (including the contribution of this paper).
A successful AMR parser will need to handle every SemEval task well, and conquer several additional semantic parsing challenges unique to AMR.
AMR also has nasty structural properties, with no guarantees about projectivity or even acyclicity of its graphs.

In our dicussion of these challenges, we follow the convention of dividing AMR parsing into two steps, first proposed by \cite{Flanigan:14}. The first step is to generate nodes from text, which we'll refer to as NER++. The second step is to link these nodes into a fully connected AMR graph, which we'll call SRL++.

A reasonable NLP researcher with a background in structure-prediction might look at AMR and assume that NER++ is easy, and the difficult part of AMR parsing is predicting the edges in these nasty cyclic non-projective graphs (SRL++).
Such a researcher might spend months testing novel structure-prediction algorithms and training regimes to capture aspects of the non-projective cyclic graphs, only to find that with a relatively simple MST, \cite{Flanigan:14} has set an extremely strong baseline on the SRL++ task, and gains are negligible at best. This is because, contrary to perfectly reasonable intuition, SRL++ is \textit{not} the hard part of AMR parsing.

The hard part of AMR parsing is NER++.
Our first piece of evidence in making this claim is a close analysis of the only published AMR parser to date, \cite{Flanigan:14}, dubbed ``JAMR''.
When JAMR makes parses using its own NER++ and SRL++ components, it gets an end-to-end score of 0.58 F1.
However, when JAMR is given a gold NER++ output, and must only perform SRL++ over given chunks, which it does with a dual-decomposition constrainted MST, it scores \textit{0.80} F1.
Inter-annotator agreement on AMR is 0.83 F1 (and annotators very rarely disagree on NER++ \todo{verify}, which amounts to disagreeing about the entities mentioned in a sentence) so this means the SRL++ component is nearly perfect, if given perfect NER++.

Upon reflection it makes sense that SRL++ is relatively easy given a perfect set of nodes to link together.
There's a strong type-check feature for the existence and type of any arc just by looking at its end-points.
If a system is considering how to link the node ``run-01'' in \reffig{glee}, the verb-sense frame for ``run-01'' leaves very little entropy in terms of what we could assign as an ARG0 arc, and it must be a noun, so either ``he'' or ``dog'', which is easily decided in favor of ``he'' by looking for an nsubj arc in the dependency parse. Given a modest amount of data, a simple parser can learn these lexical, syntactic type-check rules.

However, NER++ is extremely challenging. This task was handled by a simple baseline system in \cite{Flanigan:14}, which memorized a purely lexical mapping from spans of text to AMR nodes, augmented by an NER system and time expression regex at test time.

The primary contribution of this paper is a method to largely delexicalize NER++. Choosing from among a small set of `generative actions' our system can derive an AMR sub-graph from a span of tokens (see \reffig{method}). For example, we have an action \textit{VERB} that will perform a verb-sense-disambiguation to the appropriate PropBank frame \cite on the source token, like the ``ran'' to ``run-01'' example in \reffig{glee}.

We show this approach improves recall dramatically over previous approaches, and that end to end performance is improved 0.59 to 0.62 smatch when our generative actions are stitched together by the previous state of the art parser \cite{Flanigan:14}. We suspect that the modest improvement is due to the fact that our NER++ system is able to generate nodes that the SRL++ system has never seen during training time, which makes the lexicalized typecheck features useless.

% Previous work has observed that AMR parsing can be partitioned into two tasks: a rich lexically grounded entity detection system, which we call NER++ (see \refsec{ner}), and a relationship detection system, which we call SRL++ (see \refsec{srl}).

% As an example of the distinction, let's briefly go over \reffig{glee}, the AMR for ``he gleefully ran to his dog Rover''. To produce this parse, the NER++ task will have to do a verb sense disambiguation on ``ran'' to get ``run-01'', and a lemmatization on ``gleefully'' to ``glee''. Then NER++ will have to recognize ``Rover'' is a name, and generate the sub-graph (name :op1 ``Rover''). It will also have to recognize that ``to'' is a dropped preposition, and so doesn't get a node, and ``his'' is a coreferant that also doesn't get a node. Then ``dog'' and ``he'' will have to be generated directly as nodes. The SRL++ task is then responsible for linking together the output of the NER++ task to produce a fully-connected semantic graph.

% At test time, AMR parsing is commonly divided into two tasks, which we refer to as NER++ and SRL++.

% Valid examples to use:
% excessively -> excessive
% mother -> mother
% exhaustive -> exhaust-01

% For instance, the adjective \textit{exhaustive} in ``the \textit{exhaustive} search'' should be parsed as an AMR verb, \textit{exhaust-01}, but JAMR is unable to recognize this because it has no instance of the adjective \textit{exhaustive} used in the training data. Our system can generalize from training data to learn that \textit{exhaustive} is to be treated as a verb, and match against the OntoNotes \needcite database of verb sense frames at test time to produce \textit{exhaust-01}.

% In order to train our system, we need a correspondence between AMR nodes and their source tokens in the training data. For example, a node with the title ``glee'' will almost certainly represent the token ``gleefully'' if that appears in the source text. However, AMR training data is ``unaligned'', meaning that no effort is made to annotate which token in a sentence is being represented by a given node in an AMR graph.

% Automatically inferring alignments is a source of noise in training our system.
% We also propose a novel alignment system inspired by our generative actions to explicitly minimize that noise, using a metric we define as ``action informativeness''.
% We cast AMR alignment as the task of finding the alignment of AMR graph to the source sentence that maximizes the informativeness of the implied generative actions.
% We define this further in \refsec{alignment}.

\Section{crash}{A Crash-Course in AMR}

AMR is a language for expressing semantics as a directed (potentially cyclic) graph, where nodes represent concepts and arcs are relationships between concepts.
AMR makes no effort to have a one-to-one correspondence between nodes in a graph and tokens in the sentence whose semantics is being represented.
In fact, AMR will often expand single tokens into large sub-graph elements, or ignore tokens completely.
AMR represents the relationships between objects referred to by the surface text, \textit{not} the relationships between the words themselves.

To introduce AMR and its notation, we'll unpack the translation of the sentence ``he gleefully ran to his dog Rover''. We show in \reffig{glee} the interpretation of this sentence as an AMR graph.

Note that the root node of the graph is labeled ``run-01''. This is the name of a verb sense definition drawn from PropBank \needcite for the sense of the verb ``ran'' in this sentence.

``run-01'' has an outgoing ``ARG0'' arc to a node ``he'', with semantics (drawn from the PropBank frame) that roughly correspond to ``he'' being the doer of the ``run-01'' action. The ``run-01'' has an outgoing ``mod'' to ``glee,'' which has the catch-all semantics that ``run-01'' is somehow modified by the concept ``glee.'' ``run-01'' also has a ``destination'' arc to ``dog,'' which preposition senses from (Srikumar, 2013) \needcite, and means that the destination of the ``run-01'' action is ``dog''. Then we have a section of the graph that is best interpreted as a unit, where all of the children of ``dog'' effectively mean that ``dog'' has the name ``Rover.''

\Subsection{formal}{Formal task definition}

Formally, given an array of tokens $S = [s_0, \ldots, s_n]$, generate a directed AMR graph $G$, defined as the pair $(N = [n_0, \ldots, n_k], A \in L^{k*k})$, where $N$ is an array of AMR nodes (which doesn't have to be the same length or have a clear correspondence to $S$), and $A$ is a matrix of labels $L$, where $A_{i,j} = l \in L$ means that an arc exists from node $n_i$ to node $n_j$ with label $l$ in the parsed graph. We include the special label ``NONE'' in $L$, corresponding to no arc existing between two nodes.

\Subsection{ner}{NER++}

Parsing to the AMR representation demands a rich NER system, word sense disambiguation, number normalization, time parsing, and many semantic nominalizations and part of speech translations, and within-sentence coreference. We refer to these ``low-level'' AMR tasks collectively as NER++. NER++ is the sub-task of generating the best AMR sub-graphs (``sub-graph'' is defined in \refsec{sub-chunks}) given the set of tokens $S$. This involves both partitioning the source text into spans that will be rendered as a single sub-graph in AMR (e.g. ``run'', ``People's Republic of China'', ``January 1, 2008''), and then mapping each of those spans into a corresponding AMR sub-graph of maximum likelihood.

\Subsection{srl}{SRL++}

Given a perfect NER++ system for an AMR parser, there remains the task of noting the verb arguments, preposition sense actionging, and doing some augmented semantic dependency parsing in order to join the disjoint NER++ output into a single AMR parse. We call this task SRL++. SRL++ is the sub-task of taking as input the disjoint sub-graphs generated by NER++, and adding the maximum likelihood set of arcs between the sub-graphs in order to have a fully connected graph.

\Subsection{sub-chunks}{AMR Subgraphs}

AMR contains components that, while they may be composed of multiple nodes, can logically be considered the expression of a single concept. For the NER++ task, we would like to be able to generate these ``single concept subgraphs'' directly from spans of text.

\Fig{sailor.png}{0.25}{sailor}{AMR representation of the word ``sailor'', which is notable for breaking the word up into a self-contained multi-node unit signifying some etymological understanding.}

\Fig{date.png}{0.25}{date}{AMR representation of the span ``January 1, 2008'', an example of how AMR can represent structured data by hallucinating additional nodes like ``date-entity'' to signify the presence of special structure}

AMR makes an attempt to capture some semantic meanings in words that are difficult to capture in a way that is not domain specific. For example, the token ``sailor'' in a sentence will evoke the concept graph representing a person who performs the action ``sail-01'', see \reffig{sailor}. This is difficult to model without resorting to memorization, because the etymological clues are so sparse. We note this as an area for further exploration.

AMR can also capture structured data, like time expressions, see \reffig{date}. In dates, a ``date-entity'' node is hallucinated to signify that this cluster of nodes is part of a structured sub-component of an AMR graph, with specific semantics. Dates are a good example of a recurring pattern in AMR, which is to have an ``artificial node'' signify that all its immediate children are part of a structured piece of data, with some special interpretation. The most common example of this pattern is the ``name'' node, which signifies that its immediate children comprise the tokens of a name object.

\section{Previous Work}

Semantic parsing has been explored extensively. \todo{Cite Percy, Zettlemoyer, etc}

The twin challenges of unobserved alignments and highly non-projective, potentially cyclic structures makes AMR a novel challenge.
At the time of this writing, the JAMR parser \cite{Flanigan:14} is the only published AMR parser.
The crucial insight in JAMR is that AMR parsing can be broken into two relatively distinct tasks: interpreting what entities are being referred to in the text (which we call NER++), and then discovering what relationships those entities have between one another other (which we call SRL++).

For NER++, JAMR uses a simple Viterbi sequence model to directly generate AMR-subgraphs from memorized mappings of text spans to subgraphs. Then for SRL++ JAMR uses a variation of the maximum spanning tree algorithm augmented by dual decomposition to impose linguistically motivated constraints on a maximum likelihood stitching. JAMR's SRL++ component is extremely effective, and we were unable to produce a better SRL++ system in our experiments with several other structured prediction approaches.

\section{NER++ Method}

Our approach to improving NER++ is very simple: instead of trying to pick which of thousands of AMR sub-graphs to generate from a span of text directly, we partition the AMR sub-graph space in terms of the actions needed to derive a node from its aligned token. At test time we do a sequence labeling of input tokens with these actions, and then deterministically derive the AMR sub-graphs from spans of tokens by applying the transformation decreed by their actions. This dramatically reduces sparsity, and helps improve end-to-end performance, but is most beneficial for domain transfer. We explain in \refsec{actions} how exactly we manage this partition, and explain in \refsec{data} how we create training data from existing resources to train a action-type classifier. Then we setup the classifier itself in \refsec{classifier}.

\Subsection{actions}{Derivation actions}

We partition the AMR sub-graph space into a set of 7 actions, each corresponding to an action that will be taken by the NER++ system if a token receives this classification.

\begin{itemize}
\item \textbf{VERB}: Look for the most similar PropBank frame, make that the title of the corresponding node.
\item \textbf{IDENTITY}: Take the lowercased version of the token to be the title of the corresponding node.
\item \textbf{VALUE}: Parse the token to an integer value, and use that as the node. AMR actually does type-check, so 
\item \textbf{LEMMA}: Take the lemma of the token to be the title of the corresponding node.
\item \textbf{NONE}: Ignore this token in the final output.
\item \textbf{NAME}: Attach a hallucinated ``name'' node to the top of this span, but don't add an NER action type on top of the ``name'' node.
\item \textbf{DICT}: Look up the most probable chunk associate with this lexical span. This functions as a back off if no other actions are appropriate.
\end{itemize}

\Subsection{actions}{Notes on the DICT action}

It's not always possible to derive an AMR sub-graph directly from tokens at test time without having memorized a mapping. For example, the parse of ``sailor'' as ``person who sails'', see \reffig{sailor}, is nearly impossible without some form of memorization. That's where the \textbf{DICT} class is important.

To implement a \textbf{DICT} class, we memorize a simple mapping from spans of text, like ``sailor'' to their corresponding most frequently seen AMR sub-graphs in the training data, in this case \reffig{sailor}. At test time we can do a lookup in this dictionary for any element that gets labeled with a \textbf{DICT} action. Previous approaches have been the equivalent of labeling every node with the \textbf{DICT} action, so our reduction of its use is significant. This is the distribution of actions on the LDC2014T12 proxy training data, after our automatic alignment allows us to induce actions (see \refsec{data} for how this is done).

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Action & \bf \# Tokens & \bf \% Total \\ \hline
NONE & 41538 & 0.371\\
DICT & 30027 & 0.268 \\
IDENTITY & 19034 & 0.170 \\
VERB & 11739 & 0.104 \\
LEMMA & 5029 & 0.045 \\
NAME & 4537 & 0.04 \\
VALUE & 16  & 0.001\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Distribution of action types in the proxy section of the LDC2014T12 dataset, generated from automatically aligned data. }
\end{table}

Note that \textbf{DICT} counts for around 27\% of the training data, meaning that more than 72\% of tokens can be generated correctly by our action type classifier even if we've never seen them before, which is a huge win.

We believe that \textbf{DICT} should count for much less than 27\%, and \textbf{LEMMA} should count for much more than 4\%, but issues with existing lemmatizers prevent this, see our error analysis in \refsec{errors}.

\Subsection{informativeness}{Action Informativeness Hierarchy}

We define the concept of ``action informativeness'' of an action $a$ as the probability of deriving the correct node from a span of tokens, given that those tokens are labeled with the action $a$, and $a$ is the correct action for that span of tokens.

To provide a concrete example, our dictionary lookup classifier has a test-set accuracy of 0.67. That means that the ``action informativeness'' of the \textbf{DICT} action is 0.67, because given that we correctly label a token as \textbf{DICT}, there is a probability of 0.67 that we correctly generate the corresponding node.

In contrast to \textbf{DICT}, correctly labeling a node as \textbf{IDENTITY}, \textbf{NAME}, and \textbf{NONE} have action informativeness of 1.0, since there is no ambiguity in the node generation once one of those actions have been selected, and we are guaranteed (probability 1.0) to generate the correct node given the correct action.

\Fig{informativeness.png}{0.25}{hierarchy}{Informativeness hierarchy for action tags within AMR.}

This allows us to induce an action informativeness hierarchy, with more informative actions taking precedence over less informative actions for several important tasks. We demonstrate this hierarchy in \reffig{hierarchy}.

\Subsection{data}{Inducing Derivation actions from Training Data}

Given a set of AMR training data, in the form of (graph,sentence) pairs, we first induce alignments from the graph nodes to the sentence, see \refsec{alignment}. Given an alignment, which is an annotation on the graph noting for each node $N_i$ a token $S_j$ that is most likely to have ``generated'' $N_i$, we can induce alignments. For concreteness, imagine the token $S_j$ is ``running'', and the node $N_i$ has the title ``run-01''. For each action type, we can ask whether that action type is able to take token $S_j$ and correctly generate $N_i$. The two action types we find that are able to correctly generate this node are \textbf{DICT} and \textbf{VERB}. We choose the most informative action type of those available to generate the observed node. In this case, that means we choose \textbf{VERB}.

In general, our algorithm is as follows. For all $S_j$ to which no $N_i$ exists such that $N_i$ aligns to $S_j$, assign the action \textbf{NONE} to $S_j$. For all pairs $N_i$, $S_j$, assign $S_j$ the most informative action possible that could have generated $N_i$. \todo{Flesh out discussion of adjacent DICT nodes}

\Subsection{classifier}{Action Classifier}

We use an extremely simple max-ent classifier to make action decisions. The classifier takes as input a pair $< i, S >$, where $i$ is the index of the token in the input sentence, and $S$ is a sequence tokens representing the source sentence. The output of the classifier is a action $T$ such that the likelihood with respect to the data of token $i$ in sentence $S$ generating a node according to the action specified by $T$ is maximized. See Appendix A for a list of classifier features.

\subsection{Test Time Behavior}

At test time, given a sequence of input tokens, we do a simple classification of each token separately, to get a sequence labeling of our input tokens. Then for each token, we apply the behavior associated with the token label, and the resulting set of sub-graphs is passed on to SRL++ for linking.

\Section{alignment}{Automatic Alignment of Training Data}

AMR training data is in the form of bi-text, where we are given a set of (sentence,graph) pairs, with no explicit alignments between them. For example, imagine we are given the graph for "He gleefully ran to his dog Rover", as shown in \reffig{glee}. Although it's obvious to a human, the training data has no reference to the fact that the node ``run-01'' came from the token ``ran''. There is therefore a crucial task of generating these alignments prior to running training algorithms.

\subsection{Alignment Task Definition}

In plain english, we want a projective mapping from nodes to tokens. It is perfectly possible for multiple nodes to align to the same token. It is not possible, within our framework, to represent a single node being sourced from multiple tokens.

To define exactly what is meant by an `alignment', let there be a pair $G = <N,A>$ where $N$ is a set of nodes and $A$ is an $|N|$ by $|N|$ matrix of binary variables, representing the presence or absence of directed arcs between nodes. For example, $A_{i,j} = T$ means that an arc exists between $N_i$ and $N_j$, and $A_{i,j} = F$ means that no arc exists between $N_i$ and $N_j$. Let there be a set of tokens $S$, such that $S_i$ is the $i$th token in the source sentence. We would like an array $B$, where $|B| = |N|$, and for all $i$, $B_i$ is in the range $(1,|S|)$. For $B_i = n$, it means that token $S_n$ generated $N_i$.

\subsection{Previous Alignment Work}

There have been two previous attempts at producing automatic AMR alignments. The first was published as a necessary component of JAMR, \cite{Flanigan:14}, and used a rule-based approach to perform alignments, which worked well on the small sample of 100 hand-labeled sentences used to develop the system. The second published approach, \todo{cite short paper}, rendered AMR graphs as text, and then used traditional alignment techniques from machine translation to align tokens in the source text and nodes in the AMR graphs. This approach works reasonably well, but fails to take advanactione of the inherently graphical structure of AMR, and regularities within that structure like named entities and quantity values.

\subsection{Intuition}

Our decomposition of the AMR node generation process into a set of actions provides an interesting way to align unaligned AMR graphs. We would like an alignment of AMR nodes to the source tokens such that we maximize the ``informativeness'' of the actions that we use to generate the AMR nodes from the source text.

We can define the ``informativeness'' of a given action by the probability of generating the correct nodes given the correct sequence label. The only label with a probability of correct generation that is less than 1 (i.e. is not and immediate guaranteed win) is \textbf{DICT}, which looks up the token in a dictionary, and on our dev set less than 70\% are correctly generated from a \textbf{DICT}.

That suggests a relatively simple heuristic for producing good alignments: minimize the number of \textbf{DICT} sequence labels implied by a given alignment $A$. We would also like to constrain nodes that are not adjacent to one another to not align to the same token, except in certain cases where hallucinated AMR node structure suggests that a contiguous segment of 3 or more nodes is plausible.

\subsection{Boolean Linear Program Formulation}

We can formulate the alignment problem and constraints given above as a Boolean LP.

Let $Q$ be a matrix in $\mathcal{B}^{|N| x |S|}$ ($Q$ is a matrix of boolean variables of size $|N|$ x $|S|$). The meaning of $Q_{i,j} = \mathbbm{1}$ can be interpreted as node $N_i$ having come from token $S_j$. Furthermore, let $V$ be a matrix $\mathcal{T}^{|N| x |S|}$ ($V$ is a matrix of derivation types, a set we call $\mathcal{T}$, of size $|N|$ x $|S|$). The matrix element $V_{i,j}$ gets the derivation type that would be implied by node $N_i$ aligning to token $S_j$. \todo{Needs graphic} Our goal can then be formulated roughly as follows:

\[\sum_{i,j} Q_{i,j}*\mathbbm{1}{(V_{i,j} = DICT)}\]

We would like to constrain the alignment so that each node must align to exactly one token:

\[\forall i (\sum_{j} Q_{i,j} = 1)\]

It is also useful to prevent nodes that are not adjacent in the AMR graph, and do not have exactly the same title, from aligning to the same token. Let $\mathcal{J}$ be the set of all pairs $(k,l)$ such that $k \neq l$ and $N_k$ and $N_l$ are not adjacent in the graph, and do not have the same title. Then we can enforce this constraint with,

\[\forall (k,l) \in \mathcal{J} (\forall j (Q_{k,j} + Q_{l,j} \leq 1))\]

We also find edit distance to be a useful encouragement for nodes to align to their correct source token, so we would like to linearly augment our goal term with another value to reflect how closely our proposed alignment follows edit distance. Let $\mathcal{E}$ be a matrix in $\mathcal{R}^{|N|x|S|}$, where $E_{i,j}$ is the Jaro-Winkler edit distance between the title of node $N_i$, and the sentence token $S_j$. Then we can augment our objective function with a linear encouragement, modulated by $\alpha$, to align to the close edit-distance concepts overall. Our new augmented objective function is:

\[\sum_{i,j} Q_{i,j}*(\mathbbm{1}{(V_{i,j} = DICT)} - \alpha \mathcal{E}_{i,j})\]

We have many choices for packages that can solve this Boolean LP efficiently. We used Gurobi \needcite.

Given a matrix $Q$ that minimizes our objective, we can decode our solved alignment as follows: for each $i$, align $N_i$ to the $j$ s.t. $Q_{i,j} = 1$. By our constraints, exactly one such $j$ must exist.

\section{Results}

\subsection{End to end results}

Our end to end results are reported by plugging the output of our NER++ into the SRL++ component of JAMR \cite{Flanigan:14}, which is able to produce final AMR graphs when given a sequence of spans and their corresponding chunks. AMR parsing accuracy is measured with a metric called smatch \needcite, which stands for ``s(emantic) match''. The metric is the F1 of a best-match between triples implied by the target graph, and triples in the parsed graph. We report much higher recall, and a slightly improved F1 score.

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|l|l|r|}
\hline NER++ & Dataset & P & R & \bf F1 \\ \hline
JAMR & LDC2014T12 & \textbf{0.671} & 0.532 & 0.59 \\
\textbf{Robust} & LDC2014T12 & 0.639 & \textbf{0.596} & \textbf{0.62} \\
JAMR & LDC2014E117 & \textbf{0.669} & 0.529 & 0.59 \\
\bf Robust & LDC2014E117 & 0.636 & \textbf{0.601} & \textbf{0.62} \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Results on two AMR datasets. }
\end{table}

\todo{interpretation}

\subsection{Component results}

On our action-type sequence labeling data generated from automatic alignments on train and test splits of \textit{LDC2014T12}, our classifier achieved a test accuracy of \textbf{0.841}.

The \textbf{DICT} action lookup table achieved an accuracy of \textbf{0.67} on the same train and test set. This is remarkable, given that our model moves many of the difficult semantic tasks onto the \textbf{DICT} tag, and we are using no learning here beyond a simple count of observed span to sub-graph mappings.

\subsection{Alignment results}

We hand annotated 500 sentence graph pairs with alignments. These pairs were selected to evenly represent every domain from the LDC2014T12 dataset, and were hand-annotated over a period of three weeks by a single individual, so alignment style is consistent throughout. Hallucinated nodes, like ``temporal-entity'', are aligned to their left-most child for consistency, with the reasoning that they will then grouped in a \textbf{DICT} action with their children during training and testing.

\todo{Report Results}

% Our approach really shines in domain transfer. Using a system trained on \textbf{LDC2013E117}, which is composed of international newswire, and testing on the web forum subsection \textbf{LDC2014T12}, our system generalizes well.

\Section{errors}{Error Analysis}

\subsection{Weak lemmatization}

The \textbf{DICT} class was intended to be used for things that a system cannot know without memorization, like the nominalization of ``sailor'', see \reffig{sailor}. These don't occur nearly 25\% of the time in the training data. One of the reasons that the \textbf{DICT} class is so disappointingly large is that it's stealing from \textbf{LEMMA}, because AMR will aggressively normalize words and change their part of speech to a semantic neighbor. For example, `gleefully' gets mapped to `glee' and not `gleeful', which is hard to do automatically with stemming rules in the general case. We leave this as a direction for future work.

\subsection{DICT Classifier}

The \textbf{DICT} class is surprisingly large, and our attempts to handle node generation within the class were fairly feeble.

\section{Future Work}
\subsection{Semantically equivalent POS normalization}
The benefit of this approach could be increased by having a very strong stemmer tuned to AMR parsing, which currently doesn't exist.
\subsection{Etymological approach to node generation}
There is an opportunity to create and test etymologico-semantic approaches to parsing words like `sailor' that would benefit AMR parsing domain generalization tremendously. We envision a system that upon discovering the unseen noun `arbitrageur' at training time is able to retrieve the know verb lemma `arbitrage', and derive that the noun `arbitrageur' probably refers to an entity that is engaged in arbitrage. AMR training data provides an opportunity to perform and measure such a task in isolation, 

\section{Appendix}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf NER++ Features \\ \hline
Input token\\
Input token word embedding\\
Left token\\
Right token\\
Left bigram\\
Right bigram\\
POS\\
Left POS\\
Right POS\\
Left POS bigram\\
Right POS bigram\\
Token's dependency parent token\\
Token's dependency parent POS\\
Token's dependency parent arc name\\
Bag of outgoing dependency arcs\\
Number of outgoing dependency arcs\\
Number of outgoing dependency arcs (indicator)\\
Max JaroWinker to any lemma in PropBank\\
Closest (JaroWinkler) in PropBank\\
Token NER\\
Left NER bigram\\
Right NER bigram\\
Right NER bigram\\
Indicator for if token is a recognized AMR NER type\\
Indicator for if token is capitalized\\
Parent arc is prep\_* or appos, and parent has NER action\\
Indicator for token is pronoun\\
Indicator for token is part of a coref chain\\
Indicator for token pronoun and part of a coref chain\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} The features for the NER++ max-ent classifiers. }
\end{table}

\section*{Acknowledgments}

The acknowledgments should go immediately before the references.  Do
not number the acknowledgments section. Do not include this section
when submitting your paper for review.

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2015}

\begin{thebibliography}{}

\bibitem[\protect\citename{Flanigan \bgroup et al.\egroup}2014]{Flanigan:14}
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, Noah~A. Smith
\newblock 2014.
\newblock {\em ACL 14}, volume~1.

\bibitem[\protect\citename{Banarescu \bgroup et al.\egroup}2013]{Banarescu:13}
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider.
\newblock 2013.
\newblock {\em Proc. of the Linguistic Annotation Workshop and Iteroperability with Discourse}, volume~1.

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
